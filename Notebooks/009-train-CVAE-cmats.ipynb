{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep  1 11:59:02 EDT 2022\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEGIN GPU CHECKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gpu_id = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=str(gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:            187          17          96           1          73         167\n",
      "Swap:            11           0          11\n"
     ]
    }
   ],
   "source": [
    "!free -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 15.78173828125\n",
      "Free memory: 0.4658203125\n",
      "Used memory: 15.31591796875\n"
     ]
    }
   ],
   "source": [
    "# CHECK GPU\n",
    "import nvidia_smi\n",
    "try:\n",
    "    nvidia_smi.nvmlInit()\n",
    "\n",
    "    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(gpu_id)\n",
    "    # card id 0 hardcoded here, there is also a call to get all available card ids, so we could iterate\n",
    "\n",
    "    info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "\n",
    "    print(\"Total memory:\", (info.total/1024/1024/1024))\n",
    "    print(\"Free memory:\", (info.free/1024/1024/1024))\n",
    "    print(\"Used memory:\", (info.used/1024/1024/1024))\n",
    "\n",
    "    nvidia_smi.nvmlShutdown()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
      "CPU (s):\n",
      "0.3801320344209671\n",
      "GPU (s):\n",
      "0.030652874149382114\n",
      "GPU speedup over CPU: 12x\n"
     ]
    }
   ],
   "source": [
    "# Run GPU test\n",
    "import tensorflow as tf\n",
    "import timeit,pickle\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "  raise SystemError('GPU device not found')\n",
    "\n",
    "def cpu():\n",
    "    with tf.device('/cpu:0'):\n",
    "        random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
    "        net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
    "    return tf.math.reduce_sum(net_cpu)\n",
    "\n",
    "def gpu():\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
    "        net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
    "    return tf.math.reduce_sum(net_gpu)\n",
    "  \n",
    "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
    "cpu()\n",
    "gpu()\n",
    "\n",
    "# Run the op several times.\n",
    "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
    "      '(batch x height x width x channel). Sum of ten runs.')\n",
    "print('CPU (s):')\n",
    "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
    "print(cpu_time)\n",
    "print('GPU (s):')\n",
    "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
    "print(gpu_time)\n",
    "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END GPU CHECKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from umap import UMAP\n",
    "\n",
    "import os\n",
    "from datetime import datetime; now = datetime.now\n",
    "tqdm = partial(tqdm, position=0, leave=True) \n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "import helper_funcs;reload(helper_funcs);from helper_funcs import *\n",
    "del helper_funcs\n",
    "import make_models;reload(make_models);from make_models import *\n",
    "del make_models\n",
    "\n",
    "from IPython import display\n",
    "import sys\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "reload(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "analysis_name = 'test'\n",
    "\n",
    "# NB name for copying\n",
    "nb_name = '009-train-CVAE-cmats.ipynb'\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim1 = 16\n",
    "latent_dim2 = 16\n",
    "beta = .001\n",
    "gamma = .001\n",
    "batch_size = 32\n",
    "kernel_size = 3\n",
    "filters = 32\n",
    "intermediate_dim = 128\n",
    "nlayers = 3\n",
    "strides = 2\n",
    "learning_rate=0.001\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate,beta_1=0.9,beta_2=0.999,epsilon=1e-07,amsgrad=False,name='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "save_dir = os.path.join('../Assets/tf_weights',analysis_name)\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "print(analysis_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/df_comb_S1558.csv')\n",
    "\n",
    "# Fix site id\n",
    "#unique_values = np.unique(df['site'].values)\n",
    "#new_vals = np.arange(1,len(unique_values)+1)\n",
    "#df['site'] = [new_vals[val==unique_values][0] for val in df['site'].values]\n",
    "\n",
    "unique_values = np.unique(df['dataset'].values)\n",
    "new_vals = np.arange(1,len(unique_values)+1)\n",
    "df['dataset_id'] = [new_vals[val==unique_values][0] for val in df['dataset'].values]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmats = np.load('../Data/cmats_r51_S1502.npz')['data']\n",
    "# cmats.shape\n",
    "cmats = np.load('../Data/cmats_S1558.npz')['cmats']\n",
    "cmats = (cmats+1)/2\n",
    "cmats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SET UP DATA LOADER AND MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "class cvae_data_loader():\n",
    "    ''' this is the info'''\n",
    "    def __init__(self,cmats,df,batch_size=32):\n",
    "        \n",
    "        self.df = df\n",
    "        self.cmats = cmats\n",
    "        \n",
    "        self.n = len(df)\n",
    "        self.epoch = -1\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "        self.new_epoch()\n",
    "        self.n_batches = int(np.floor(min((len(self.asd_idxs),len(self.td_idxs)))/self.batch_size))\n",
    "        \n",
    "    def new_epoch(self):\n",
    "        \n",
    "        self.asd_idxs = np.nonzero((self.df['diag'].values==1))[0]\n",
    "        self.td_idxs = np.nonzero((self.df['diag'].values==2))[0]\n",
    "        \n",
    "        self.asd_idxs = np.random.permutation(self.asd_idxs)\n",
    "        self.td_idxs = np.random.permutation(self.td_idxs)\n",
    "        \n",
    "        self.epoch += 1\n",
    "        self.b = 0\n",
    "        \n",
    "        \n",
    "    def get_batch(self):\n",
    "        self.b += 1\n",
    "        \n",
    "        if self.b==self.n_batches:\n",
    "            self.new_epoch()\n",
    "        \n",
    "        self.batch_asd_idx = self.asd_idxs[np.arange(self.b*self.batch_size,self.b*self.batch_size+self.batch_size)]\n",
    "        self.batch_td_idx = self.td_idxs[np.arange(self.b*self.batch_size,self.b*self.batch_size+self.batch_size)]\n",
    "        \n",
    "        self.batch_asd = self.cmats[self.batch_asd_idx,:,:]\n",
    "        self.batch_td = self.cmats[self.batch_td_idx,:,:]\n",
    "        \n",
    "        self.batch_df = self.df.iloc[np.hstack((data_loader.batch_asd_idx,data_loader.batch_td_idx))]\n",
    "        \n",
    "        return self.batch_asd,self.batch_td,self.batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Data Loader\n",
    "data_loader = cvae_data_loader(cmats=cmats, df=df, batch_size=32)\n",
    "batch_asd,batch_td,batch_df = data_loader.get_batch()\n",
    "batch_asd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(batch_asd.min(),batch_asd.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = np.hstack((len(df),batch_asd.shape[1:]))\n",
    "data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvae_dashboard(red='PCA'):        \n",
    "    #prediction = cvae.predict(patient_batch)    \n",
    "    predictions = cvae.predict([patient_batch,control_batch])\n",
    "    sigma = (np.e ** z_encoder.predict(patient_batch)[1]).mean()\n",
    "    sigmas.append(sigma)\n",
    "\n",
    "    mu = z_encoder.predict(patient_batch)[0]\n",
    "    mus.append(np.mean([mu[:,0].std() for i in range(mu.shape[1])]))\n",
    "\n",
    "    prediction = predictions[0]\n",
    "\n",
    "    cmat_actual = np.corrcoef(np.vstack((patient_batch.reshape(patient_batch.shape[0],-1),control_batch.reshape(control_batch.shape[0],-1))))\n",
    "    cmat_pred = np.corrcoef(np.vstack((predictions[0].reshape(predictions[0].shape[0],-1),predictions[1].reshape(predictions[1].shape[0],-1))))\n",
    "    c_sim.append(np.corrcoef(get_triu(cmat_pred),get_triu(cmat_actual))[0,1])\n",
    "\n",
    "\n",
    "    ### PROGRESS PLOTTING\n",
    "    display.clear_output(wait=True);\n",
    "    display.display(plt.gcf());\n",
    "    #Organise figure\n",
    "    ncols = 4;nrows=7\n",
    "    if np.mod(i,5)==0:\n",
    "        plt.close()\n",
    "    plt.subplots(nrows,ncols,figsize=(15,15)); # MAKE THE FIGURE\n",
    "\n",
    "\n",
    "    ##### SUBPLOT 1 & 2 ##### \n",
    "\n",
    "    plt.subplot(nrows,ncols/2,1) # PLOT LOSS\n",
    "    xs = np.arange(len(loss))+1\n",
    "    m,b = np.polyfit(xs,loss,deg=1)\n",
    "    plt.plot(loss)\n",
    "    plt.plot(xs, m*xs + b)\n",
    "    plt.title(f'Epoch {data_loader.epoch} batch {data_loader.b}/{data_loader.n_batches} | Loss {loss[-1]:.2f}, beta: {m:.4f}')\n",
    "\n",
    "    ##### SUBPLOT 3 ##### \n",
    "    plt.subplot(nrows,ncols,3) # PLOT LOSS LAST 50\n",
    "    hb = 50\n",
    "    if len(loss)>hb:\n",
    "        plot_loss = loss[-hb::]\n",
    "        xs = np.arange(len(plot_loss))\n",
    "        m,b = np.polyfit(xs,plot_loss,deg=1)\n",
    "        plt.plot(plot_loss)\n",
    "        plt.plot(xs, m*xs + b)\n",
    "        #plt.title(hist)\n",
    "        plt.title(f'Loss last {hb} it, beta {m:.4f}')\n",
    "\n",
    "    ##### SUBPLOT 4 ##### \n",
    "    plt.subplot(nrows,ncols,4)\n",
    "    plt.hist(prediction[0,:,:,0].flatten(),alpha=.5)\n",
    "    plt.hist(patient_batch[0,:,:].flatten(),alpha=.5)\n",
    "    plt.legend(['predicted','actual'])\n",
    "    plt.title('in/out histograms')\n",
    "\n",
    "    ##### SUBPLOT 5 ##### \n",
    "    plt.subplot(nrows,ncols,5) #RSA over time\n",
    "    plt.plot(c_sim)\n",
    "    plt.title(f'in/out RSA: {c_sim[-1].round(2)}')\n",
    "\n",
    "    if len(c_sim)>5: # PLOT LS LINE\n",
    "        xs = np.arange(len(c_sim))+1\n",
    "        m,b = np.polyfit(xs,c_sim,deg=1)\n",
    "        plt.plot(xs, m*xs + b)\n",
    "        plt.title(f'in/out RSA: {c_sim[-1].round(2)}, b={m:.4f}')\n",
    "\n",
    "\n",
    "    ##### SUBPLOT 6 ##### \n",
    "    plt.subplot(nrows,ncols,6)\n",
    "    if len(c_sim)>hb:\n",
    "        #plot_loss = loss[-hb::]\n",
    "        xs = np.arange(len(c_sim[-hb::]))\n",
    "        m,b = np.polyfit(xs,c_sim[-hb::],deg=1)\n",
    "        plt.plot(c_sim[-hb::])\n",
    "        plt.plot(xs, m*xs + b)\n",
    "        #plt.title(hist)\n",
    "        plt.title(f'in/outRSA last {hb} it, b={m:.4f}')\n",
    "\n",
    "    # ##### SUBPLOT 7 ##### \n",
    "    # plt.subplot(nrows,ncols,7)\n",
    "    # lbls = ['age','sex','fiq','dsm','site','ados'];\n",
    "    # xs = np.arange(len(lbls));\n",
    "    # plt.bar(xs,batch_rsas[0:6]);\n",
    "    # plt.xticks(xs,labels=lbls);\n",
    "    # plt.title('S RSA')\n",
    "\n",
    "    # ##### SUBPLOT 8 ##### \n",
    "    # plt.subplot(nrows,ncols,8)\n",
    "    # lbls = ['age','sex','fiq','dsm','site','ados'];\n",
    "    # xs = np.arange(len(lbls));\n",
    "    # plt.bar(xs,batch_rsas[6::]);\n",
    "    # plt.xticks(xs,labels=lbls);\n",
    "    # plt.title('Z RSA')\n",
    "    \n",
    "    plt.subplot(nrows,ncols,8)\n",
    "    #keys = ['dataset','site','age','sex','fiq','ados_total','ados_social','ados_comm','ados_rrb',]\n",
    "    keys = ['dataset_id','siteID','age','gender','fiq','ados_total','ados_social','ados_comm','ados_rrb',]\n",
    "    scales = ['ordinal','ordinal','ratio','ordinal','ratio','ratio','ratio','ratio','ratio',]\n",
    "    rsa_res = np.array([key_rsa_cvae(keys[i],scales[i]) for i in range(len(keys))])\n",
    "    plt.plot(rsa_res[:,0],'.',markersize=15,alpha=.5)\n",
    "    plt.plot(rsa_res[:,1],'.',markersize=15,alpha=.5)\n",
    "    plt.legend(['Z','S']);\n",
    "    plt.xticks(np.arange(rsa_res.shape[0]),labels=keys,rotation=45);\n",
    "    plt.title('RSA')\n",
    "\n",
    "\n",
    "    ##### SUBPLOT 9 ##### \n",
    "    plt.subplot(nrows,ncols,9)\n",
    "    plt.plot(sigmas)\n",
    "    plt.title(f'sigmas | {sigmas[-1]:.4f}')\n",
    "\n",
    "\n",
    "    ##### SUBPLOT 10 ##### \n",
    "    plt.subplot(nrows,ncols,10)\n",
    "    plt.plot(mus)\n",
    "    plt.title(f'Mu variance {mus[-1]:.4f}')\n",
    "\n",
    "\n",
    "    ##### SUBPLOT 11 ##### \n",
    "    plt.subplot(nrows,ncols,11)\n",
    "    sns.heatmap(cmat_actual,xticklabels=[],yticklabels=[])\n",
    "    plt.title('input RSA')\n",
    "\n",
    "    ##### SUBPLOT 12 ##### \n",
    "    plt.subplot(nrows,ncols,12)\n",
    "    sns.heatmap(cmat_pred,xticklabels=[],yticklabels=[])\n",
    "    plt.title('output RSA')\n",
    "\n",
    "    # #############################################\n",
    "    # ###################Reconstructions###########\n",
    "    # #############################################\n",
    "\n",
    "    ##### SUBPLOT 13 #####     \n",
    "    rand_sub = np.random.randint(low=0,high=patient_batch.shape[0])\n",
    "\n",
    "    plt.subplot(nrows,ncols,13)\n",
    "    sns.heatmap(patient_batch[rand_sub,:,:])\n",
    "    plt.xticks([]);plt.yticks([]);plt.title('actual')\n",
    "    ##### SUBPLOT 14 #####     \n",
    "    plt.subplot(nrows,ncols,14)\n",
    "    sns.heatmap(prediction[rand_sub,:,:,0])\n",
    "    plt.xticks([]);plt.yticks([]);plt.title('predicted')\n",
    "    # ##### SUBPLOT 15 #####     \n",
    "    plt.subplot(nrows,ncols,15)\n",
    "    sns.heatmap(abs(patient_batch[rand_sub,:,:]-prediction[rand_sub,:,:,0]))\n",
    "    plt.xticks([]);plt.yticks([]);plt.title('difference')\n",
    "\n",
    "\n",
    "    # ##### SUBPLOT 16 #####                                             \n",
    "    # plt.subplot(nrows,ncols,16)\n",
    "    # plt.imshow(np.rot90(prediction[rand_sub,16,:,:,rand_map]))\n",
    "    # plt.xticks([]);plt.yticks([]);plt.title('predicted')\n",
    "    # ##### SUBPLOT 17 #####     \n",
    "    # plt.subplot(nrows,ncols,17)\n",
    "    # plt.imshow(np.rot90(patient_batch[rand_sub,6,:,:,rand_map]))\n",
    "    # plt.xticks([]);plt.yticks([]);plt.title('actual')\n",
    "    # ##### SUBPLOT 18 #####                                             \n",
    "    # plt.subplot(nrows,ncols,18)\n",
    "    # plt.imshow(np.rot90(prediction[rand_sub,6,:,:,rand_map]))\n",
    "    # plt.xticks([]);plt.yticks([]);plt.title('predicted')\n",
    "    # ##### SUBPLOT 19 #####     \n",
    "    # plt.subplot(nrows,ncols,19)\n",
    "    # plt.imshow(patient_batch[rand_sub,:,:,16,rand_map])\n",
    "    # plt.xticks([]);plt.yticks([]);plt.title('actual')\n",
    "    # ##### SUBPLOT 20 #####     \n",
    "    # plt.subplot(nrows,ncols,20)\n",
    "    # plt.imshow(prediction[rand_sub,:,:,16,rand_map])\n",
    "    # plt.xticks([]);plt.yticks([]);plt.title('predicted')\n",
    "\n",
    "\n",
    "    # #############################################\n",
    "    # ################### LOSSES ##################\n",
    "    # #############################################\n",
    "\n",
    "\n",
    "    predictions = cvae.predict([patient_batch,control_batch])\n",
    "    input_shape = data_size[1:]\n",
    "    reconstruction_loss = tf.keras.losses.mse(K.flatten(patient_batch), K.flatten(predictions[0])) \n",
    "    reconstruction_loss += tf.keras.losses.mse(K.flatten(control_batch), K.flatten(predictions[1])) \n",
    "    reconstruction_loss *= input_shape[0] * input_shape[1]\n",
    "\n",
    "\n",
    "    tg_z_mean, tg_z_log_var, tg_z = z_encoder.predict(patient_batch)\n",
    "    tg_s_mean, tg_s_log_var, tg_s = s_encoder.predict(patient_batch)\n",
    "\n",
    "    bg_z_mean, bg_z_log_var, bg_z = z_encoder.predict(control_batch)\n",
    "\n",
    "    kl_loss1 = 1 + tg_z_log_var - tf.keras.backend.square(tg_z_mean) - tf.keras.backend.exp(tg_z_log_var)\n",
    "    kl_loss2 = 1 + tg_s_log_var - tf.keras.backend.square(tg_s_mean) - tf.keras.backend.exp(tg_s_log_var)\n",
    "    kl_loss3 = 1 + bg_z_log_var - tf.keras.backend.square(bg_z_mean) - tf.keras.backend.exp(bg_z_log_var)\n",
    "\n",
    "    kl_loss1 = tf.keras.backend.sum(kl_loss1, axis=-1)\n",
    "    kl_loss2 = tf.keras.backend.sum(kl_loss2, axis=-1)\n",
    "    kl_loss3 = tf.keras.backend.sum(kl_loss3, axis=-1)\n",
    "    kl_loss = kl_loss1+kl_loss2+kl_loss3\n",
    "    kl_loss *= -0.5\n",
    "\n",
    "\n",
    "    discriminator = Dense(1, activation='sigmoid')\n",
    "    z1 = Lambda(lambda x: x[:int(batch_size/2),:])(tg_z)\n",
    "    z2 = Lambda(lambda x: x[int(batch_size/2):,:])(tg_z)\n",
    "    s1 = Lambda(lambda x: x[:int(batch_size/2),:])(tg_s)\n",
    "    s2 = Lambda(lambda x: x[int(batch_size/2):,:])(tg_s)\n",
    "\n",
    "    q_bar = tf.keras.layers.concatenate(\n",
    "      [tf.keras.layers.concatenate([s1, z2], axis=1),\n",
    "      tf.keras.layers.concatenate([s2, z1], axis=1)],\n",
    "      axis=0)\n",
    "\n",
    "    q = tf.keras.layers.concatenate(\n",
    "      [tf.keras.layers.concatenate([s1, z1], axis=1),\n",
    "      tf.keras.layers.concatenate([s2, z2], axis=1)],\n",
    "      axis=0)\n",
    "\n",
    "    q_bar_score = (discriminator(q_bar)+.1) *.85 # +.1 * .85 so that it's 0<x<1\n",
    "    q_score = (discriminator(q)+.1) *.85 \n",
    "    tc_loss = K.log(q_score / (1 - q_score)) \n",
    "    discriminator_loss = - K.log(q_score) - K.log(1 - q_bar_score)\n",
    "    discriminator_loss\n",
    "\n",
    "    loss_mse.append(reconstruction_loss.numpy())\n",
    "    loss_kl.append(kl_loss.numpy().mean())\n",
    "    loss_dc.append(tc_loss.numpy().mean())\n",
    "    loss_tc.append(discriminator_loss.numpy().mean())\n",
    "\n",
    "\n",
    "    plt.subplot(nrows,ncols,21) # MSE \n",
    "    plt.plot(loss_mse)\n",
    "    plt.title(f'MSE | {loss_mse[-1]:.4f}')\n",
    "\n",
    "    plt.subplot(nrows,ncols,22) # KL loss\n",
    "    plt.plot(loss_kl)\n",
    "    plt.title(f'KL | {loss_kl[-1]:.4f}')    \n",
    "\n",
    "\n",
    "    plt.subplot(nrows,ncols,23) # TC     \n",
    "    plt.plot(loss_tc)\n",
    "    plt.title(f'Total Correlation loss | {loss_tc[-1]:.4f}')    \n",
    "\n",
    "\n",
    "    plt.subplot(nrows,ncols,24) # Disc         \n",
    "    plt.plot(loss_dc)\n",
    "    plt.title(f'discriminator_loss | {loss_dc[-1]:.4f}')    \n",
    "\n",
    "\n",
    "    tg_s = s_encoder.predict(patient_batch)\n",
    "    tg_z = z_encoder.predict(patient_batch)\n",
    "    bg_z = z_encoder.predict(control_batch)\n",
    "\n",
    "    plt.subplot(nrows,ncols,25)\n",
    "    plt.hist(tg_s[2].flatten(),alpha=.5);\n",
    "    plt.hist(tg_z[2].flatten(),alpha=.5);\n",
    "    plt.hist(bg_z[2].flatten(),alpha=.5);\n",
    "    plt.legend(['tg_s','tg_z','bg_z'])\n",
    "    plt.title('Z')\n",
    "\n",
    "\n",
    "    plt.subplot(nrows,ncols,26)\n",
    "    plt.hist(tg_s[0].flatten(),alpha=.5);\n",
    "    plt.hist(tg_z[0].flatten(),alpha=.5);\n",
    "    plt.hist(bg_z[0].flatten(),alpha=.5);\n",
    "    plt.legend(['tg_s','tg_z','bg_z'])\n",
    "    plt.title('Mus')\n",
    "\n",
    "\n",
    "    plt.subplot(nrows,ncols,27)\n",
    "    plt.hist(tg_s[1].flatten(),alpha=.5);\n",
    "    plt.hist(tg_z[1].flatten(),alpha=.5);\n",
    "    plt.hist(bg_z[1].flatten(),alpha=.5);\n",
    "    plt.legend(['tg_s','tg_z','bg_z'])\n",
    "    plt.title('Sigmas')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_rsa_cvae(key,data_scale):\n",
    "    \n",
    "    Z = z_encoder.predict(pad2d(cmats[:,:,:,np.newaxis]))[0]\n",
    "    S = s_encoder.predict(pad2d(cmats[:,:,:,np.newaxis]))[0]\n",
    "    rsa_df = df.copy()\n",
    "\n",
    "    patients = rsa_df['diag'].values==1\n",
    "    Z = Z[patients,:]\n",
    "    S = S[patients,:]\n",
    "    rsa_df = rsa_df.iloc[patients]\n",
    "    \n",
    "    vec = rsa_df[key].values\n",
    "    e = np.isnan(vec)\n",
    "    z_fit = fit_rsa(make_RDM(vec[~e],data_scale=data_scale, metric='euclidean'),make_RDM(Z[~e],data_scale='ratio', metric='euclidean'))\n",
    "    s_fit = fit_rsa(make_RDM(vec[~e],data_scale=data_scale, metric='euclidean'),make_RDM(S[~e],data_scale='ratio', metric='euclidean'))\n",
    "    \n",
    "    return (z_fit,s_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import mse\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_fMRI_CVAE_3D(input_shape=(51,51,1),\n",
    "                     latent_dim=[2,2],\n",
    "                     beta=1,\n",
    "                     disentangle=False,\n",
    "                     gamma=1,\n",
    "                     bias=True,\n",
    "                     batch_size = 32,\n",
    "                     kernel_size = 3,\n",
    "                     filters = 16,\n",
    "                     intermediate_dim = 128,\n",
    "                     nlayers = 2,\n",
    "                     strides = 2,\n",
    "                     learning_rate=0.001,\n",
    "                     opt=None):\n",
    "    \n",
    "    \n",
    "    ndim_bg = latent_dim[0]\n",
    "    ndim_sl = latent_dim[1]\n",
    "    \n",
    "    image_size, _, channels = input_shape\n",
    "\n",
    "    kernel_regularizer=regularizers.l2(.0001)\n",
    "\n",
    "    # build encoder model\n",
    "    tg_inputs = Input(shape=input_shape, name='tg_inputs')\n",
    "    bg_inputs = Input(shape=input_shape, name='bg_inputs')\n",
    "    \n",
    "    BatchNorm = tf.keras.layers.BatchNormalization(\n",
    "    axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,\n",
    "    beta_initializer='zeros', gamma_initializer='ones',\n",
    "    moving_mean_initializer='zeros',\n",
    "    moving_variance_initializer='ones', beta_regularizer=None,\n",
    "    gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "\n",
    "    #kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0,stddev=5)\n",
    "    kernel_initializer = tf.keras.initializers.RandomUniform()\n",
    "\n",
    "    # generate latent vector Q(z|X)\n",
    "    \n",
    "    \n",
    "    z_h_layer = Dense(intermediate_dim,activation='relu', use_bias=bias,kernel_regularizer=kernel_regularizer)\n",
    "    z_mean_layer = Dense(ndim_bg, name='z_mean', use_bias=bias,kernel_regularizer=kernel_regularizer)\n",
    "    z_log_var_layer = Dense(ndim_bg, name='z_log_var', use_bias=bias,kernel_regularizer=kernel_regularizer)\n",
    "    z_layer = Lambda(sampling, output_shape=(ndim_bg,), name='z')\n",
    "\n",
    "    def z_encoder_func(inputs):\n",
    "        z_h = inputs\n",
    "\n",
    "        these_filters = filters\n",
    "        for i in range(nlayers):\n",
    "            these_filters *= 2\n",
    "            #print(these_filters)\n",
    "            z_h = Conv2D(filters=these_filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    activation='relu',\n",
    "                    strides=strides,\n",
    "                    padding='same',\n",
    "                    use_bias=bias,\n",
    "                    kernel_regularizer=kernel_regularizer)(z_h)\n",
    "        \n",
    "        # shape info needed to build decoder model\n",
    "        shape = K.int_shape(z_h)\n",
    "        z_h = Flatten()(z_h)\n",
    "        z_h = z_h_layer(z_h)\n",
    "        z_mean =  z_mean_layer(z_h)\n",
    "        #z_mean = BatchNorm(z_mean)\n",
    "        \n",
    "        z_log_var =  z_log_var_layer(z_h)\n",
    "        z = z_layer([z_mean, z_log_var])\n",
    "        return z_mean, z_log_var, z, shape\n",
    "\n",
    "    tg_z_mean, tg_z_log_var, tg_z, shape_z = z_encoder_func(tg_inputs)\n",
    "\n",
    "    # generate latent vector Q(z|X)\n",
    "    s_h_layer = Dense(intermediate_dim, activation='relu', use_bias=bias,kernel_regularizer=kernel_regularizer)\n",
    "    s_mean_layer = Dense(ndim_sl, name='s_mean', use_bias=bias,kernel_regularizer=kernel_regularizer)\n",
    "    s_log_var_layer = Dense(ndim_sl, name='s_log_var', use_bias=bias,kernel_regularizer=kernel_regularizer)\n",
    "    s_layer = Lambda(sampling, output_shape=(ndim_sl,), name='s')\n",
    "\n",
    "    def s_encoder_func(inputs):\n",
    "        s_h = inputs\n",
    "        these_filters = filters\n",
    "        for i in range(nlayers):\n",
    "            these_filters *= 2\n",
    "            s_h = Conv2D(filters=these_filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    activation='relu',\n",
    "                    strides=strides,\n",
    "                    use_bias=bias,\n",
    "                    kernel_regularizer=kernel_regularizer,\n",
    "                    padding='same')(s_h)\n",
    "        \n",
    "        # shape info needed to build decoder model\n",
    "        shape = K.int_shape(s_h)\n",
    "        s_h = Flatten()(s_h)\n",
    "        s_h = s_h_layer(s_h)\n",
    "        s_mean =  s_mean_layer(s_h)\n",
    "        #s_mean = BatchNorm(s_mean)\n",
    "        \n",
    "        s_log_var =  s_log_var_layer(s_h)        \n",
    "        s = s_layer([s_mean, s_log_var])\n",
    "        \n",
    "        return s_mean, s_log_var, s, shape\n",
    "\n",
    "    tg_s_mean, tg_s_log_var, tg_s, shape_s = s_encoder_func(tg_inputs)\n",
    "    bg_z_mean, bg_z_log_var, bg_z, _ = z_encoder_func(bg_inputs) # Aidas and Stefano team hax\n",
    "    \n",
    "    \n",
    "    # instantiate encoder models\n",
    "    z_encoder = tf.keras.models.Model(tg_inputs, [tg_z_mean, tg_z_log_var, tg_z], name='z_encoder')\n",
    "    s_encoder = tf.keras.models.Model(tg_inputs, [tg_s_mean, tg_s_log_var, tg_s], name='s_encoder')\n",
    "\n",
    "\n",
    "    # build decoder model\n",
    "    latent_inputs = Input(shape=(ndim_bg+ndim_sl,), name='z_sampling')\n",
    "\n",
    "    x = Dense(intermediate_dim, activation='relu', use_bias=bias,kernel_regularizer=kernel_regularizer,kernel_initializer=kernel_initializer)(latent_inputs)\n",
    "    x = Dense(shape_z[1] * shape_z[2] * shape_z[3], activation='relu', use_bias=bias,kernel_regularizer=kernel_regularizer,kernel_initializer=kernel_initializer)(x)\n",
    "    x = Reshape((shape_z[1], shape_z[2], shape_z[3]))(x)\n",
    "\n",
    "    these_filters = filters*(2**nlayers)/2\n",
    "    for i in range(nlayers-1):\n",
    "        x = Conv2DTranspose(filters=these_filters,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='relu',\n",
    "                          strides=strides,\n",
    "                          use_bias=bias,\n",
    "                          kernel_regularizer=kernel_regularizer,\n",
    "                          padding='same')(x)\n",
    "        these_filters //= 2\n",
    "\n",
    "    outputs = Conv2DTranspose(filters=channels,\n",
    "                            kernel_size=kernel_size,\n",
    "                            activation='sigmoid',\n",
    "                            padding='same',\n",
    "                            strides=strides,\n",
    "                            use_bias=bias,\n",
    "                            kernel_regularizer=kernel_regularizer,\n",
    "                            name='decoder_output')(x)\n",
    "\n",
    "    # instantiate decoder model\n",
    "    cvae_decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "      # decoder.summary()\n",
    "\n",
    "    def zeros_like(x):\n",
    "        return tf.zeros_like(x)\n",
    "\n",
    "    tg_outputs = cvae_decoder(tf.keras.layers.concatenate([tg_z, tg_s], -1))\n",
    "    zeros = tf.keras.layers.Lambda(zeros_like)(tg_s)\n",
    "\n",
    "    bg_outputs = cvae_decoder(tf.keras.layers.concatenate([bg_z, zeros], -1)) # Aidas look into this, is this correct\n",
    "\n",
    "    cvae = tf.keras.models.Model(inputs=[tg_inputs, bg_inputs], \n",
    "                                  outputs=[tg_outputs, bg_outputs],\n",
    "                                  name='contrastive_vae')\n",
    "\n",
    "#     cvae_fg = tf.keras.models.Model(inputs=tg_inputs, \n",
    "#                                   outputs=fg_outputs, \n",
    "#                                   name='contrastive_vae_fg')\n",
    "\n",
    "    if disentangle:\n",
    "        discriminator = Dense(1, activation='sigmoid')\n",
    "\n",
    "        z1 = Lambda(lambda x: x[:int(batch_size/2),:])(tg_z)\n",
    "        z2 = Lambda(lambda x: x[int(batch_size/2):,:])(tg_z)\n",
    "        s1 = Lambda(lambda x: x[:int(batch_size/2),:])(tg_s)\n",
    "        s2 = Lambda(lambda x: x[int(batch_size/2):,:])(tg_s)\n",
    "\n",
    "        q_bar = tf.keras.layers.concatenate(\n",
    "          [tf.keras.layers.concatenate([s1, z2], axis=1),\n",
    "          tf.keras.layers.concatenate([s2, z1], axis=1)],\n",
    "          axis=0)\n",
    "\n",
    "        q = tf.keras.layers.concatenate(\n",
    "          [tf.keras.layers.concatenate([s1, z1], axis=1),\n",
    "          tf.keras.layers.concatenate([s2, z2], axis=1)],\n",
    "          axis=0)\n",
    "\n",
    "        q_bar_score = (discriminator(q_bar)+.1) *.85 # +.1 * .85 so that it's 0<x<1\n",
    "        q_score = (discriminator(q)+.1) *.85 \n",
    "        tc_loss = K.log(q_score / (1 - q_score)) \n",
    "        discriminator_loss = - K.log(q_score) - K.log(1 - q_bar_score)\n",
    "    else:\n",
    "        tc_loss = 0\n",
    "        discriminator_loss = 0\n",
    "\n",
    "\n",
    "    reconstruction_loss = tf.keras.losses.mse(K.flatten(tg_inputs), K.flatten(tg_outputs)) \n",
    "    reconstruction_loss += tf.keras.losses.mse(K.flatten(bg_inputs), K.flatten(bg_outputs)) \n",
    "    reconstruction_loss *= input_shape[0] * input_shape[1] * input_shape[2]\n",
    "\n",
    "    kl_loss1 = 1 + tg_z_log_var - tf.keras.backend.square(tg_z_mean) - tf.keras.backend.exp(tg_z_log_var)\n",
    "    kl_loss2 = 1 + tg_s_log_var - tf.keras.backend.square(tg_s_mean) - tf.keras.backend.exp(tg_s_log_var)\n",
    "    kl_loss3 = 1 + bg_z_log_var - tf.keras.backend.square(bg_z_mean) - tf.keras.backend.exp(bg_z_log_var)\n",
    "\n",
    "    kl_loss1 = tf.keras.backend.sum(kl_loss1, axis=-1)\n",
    "    kl_loss2 = tf.keras.backend.sum(kl_loss2, axis=-1)\n",
    "    kl_loss3 = tf.keras.backend.sum(kl_loss3, axis=-1)\n",
    "\n",
    "    kl_loss = kl_loss1+kl_loss2+kl_loss3\n",
    "    #kl_loss = tf.keras.backend.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    \n",
    "    cvae_loss = tf.keras.backend.mean(reconstruction_loss + beta*kl_loss + gamma*tc_loss + discriminator_loss)\n",
    "    cvae.add_loss(cvae_loss)\n",
    "    \n",
    "    if type(opt)==type(None):\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate,beta_1=0.9,beta_2=0.999,epsilon=1e-07,amsgrad=False,name='Adam')\n",
    "        #opt = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9, momentum=0.9, epsilon=1e-07, centered=False, name='RMSprop')\n",
    "        #opt = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.1, nesterov=False, name='SGD')\n",
    "\n",
    "    cvae.compile(optimizer=opt,run_eagerly=True)\n",
    "    \n",
    "    return cvae, z_encoder, s_encoder, cvae_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import make_models;reload(make_models);from make_models import *\n",
    "batch_size = 32\n",
    "\n",
    "cvae, z_encoder, s_encoder, cvae_decoder = get_fMRI_CVAE_3D(input_shape=(64,64,1),\n",
    "                                                             latent_dim=[latent_dim1,latent_dim2],\n",
    "                                                             beta=beta,\n",
    "                                                             gamma=gamma,\n",
    "                                                             disentangle=True,\n",
    "                                                             bias=True,\n",
    "                                                             batch_size = batch_size,\n",
    "                                                             kernel_size = kernel_size,\n",
    "                                                             filters = filters,\n",
    "                                                             intermediate_dim = intermediate_dim,\n",
    "                                                             nlayers = nlayers,\n",
    "                                                             strides = strides,\n",
    "                                                             learning_rate=learning_rate,\n",
    "                                                             opt=opt)\n",
    "\n",
    "num_params = np.sum([np.prod(val.get_shape()) for val in cvae.trainable_weights])\n",
    "print(f'# params| {num_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_decoder.summary(line_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_encoder.summary(line_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_encoder.summary(line_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,all_rsas,sigmas,mus,c_sim = [],[],[],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mse,loss_kl,loss_dc,loss_tc = [],[],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad2d = tf.keras.layers.ZeroPadding2D(padding=((6,7),(6,7))) #If tuple of 2 tuples of 2 ints: interpreted as ((top_pad, bottom_pad), (left_pad, right_pad))\n",
    "data_loader = cvae_data_loader(cmats=pad2d(cmats[:,:,:,np.newaxis])[:,:,:,0].numpy(), df=df, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depad(padded_cmats):\n",
    "    if padded_cmats.ndim==3:\n",
    "        depadded = padded_cmats[:,6:57,6:57]\n",
    "    elif padded_cmats.ndim==4:\n",
    "        depadded = padded_cmats[:,6:57,6:57,:]\n",
    "    else:\n",
    "        raise Exception('dims not implemented, wtf')\n",
    "    return depadded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_batch,control_batch,batch_df = data_loader.get_batch() # Get a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train==True:\n",
    "    for epoch in tqdm(range(5000+1)):\n",
    "        for i in range(data_loader.n_batches):\n",
    "\n",
    "            patient_batch,control_batch,batch_df = data_loader.get_batch() # Get a batch\n",
    "\n",
    "            hist = cvae.train_on_batch([patient_batch[:,:,:,np.newaxis],control_batch[:,:,:,np.newaxis]]) # pass a batch\n",
    "            assert not np.isnan(hist),'loss is NaN - you f**cked up'  # check nothing crashed\n",
    "            loss.append(hist) # keep track of loss\n",
    "\n",
    "            if all((i==0,np.mod(epoch,25)==0)):\n",
    "                cvae_dashboard() # plot training progress\n",
    "                cvae.save_weights(os.path.join(save_dir,'cvae_weights')) # SAVE WEIGHTS\n",
    "                np.save(os.path.join(save_dir,'cvae_loss.npy'),np.array(loss)) # Save loss            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save latent representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvae_predict(z,s,asd):\n",
    "    \n",
    "    assert z.shape[0]==s.shape[0],'bad'\n",
    "    \n",
    "    if np.array(asd).ndim==0:\n",
    "        asd = np.repeat(asd,z.shape[0])\n",
    "        \n",
    "    z_ = np.zeros(s.shape)\n",
    "    s[~asd,:] = 0\n",
    "    \n",
    "    l = np.hstack((z,s))    \n",
    "    recon = cvae_decoder.predict(l)\n",
    "    \n",
    "    return recon[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = df['diag'].values==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmats_padded = pad2d(cmats[:,:,:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_mu,Z_sigma,Z = z_encoder.predict(cmats_padded)\n",
    "S_mu,S_sigma,S = s_encoder.predict(cmats_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_sample100 = np.array([z_encoder.predict(cmats_padded)[2] for _ in tqdm(range(100))])\n",
    "S_sample100 = np.array([s_encoder.predict(cmats_padded)[2] for _ in tqdm(range(100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_td_mu = cvae_predict(Z_mu[~patients,:],S_mu[~patients,:],asd=False)\n",
    "recon_asd_mu = cvae_predict(Z_mu[patients,:],S_mu[patients,:],asd=True)\n",
    "recon_twin_mu = cvae_predict(Z_mu[patients,:],S_mu[patients,:],asd=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_td_mu = depad(recon_td_mu)\n",
    "recon_asd_mu = depad(recon_asd_mu)\n",
    "recon_twin_mu = depad(recon_twin_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_td_samples = np.array([cvae_predict(Z_sample100[i,~patients,:],S_sample100[i,~patients,:],asd=False) for i in tqdm(range(Z_sample100.shape[0]))])\n",
    "recon_asd_samples = np.array([cvae_predict(Z_sample100[i,patients,:],S_sample100[i,patients,:],asd=True) for i in tqdm(range(Z_sample100.shape[0]))])\n",
    "recon_twin_samples = np.array([cvae_predict(Z_sample100[i,patients,:],S_sample100[i,patients,:],asd=False) for i in tqdm(range(Z_sample100.shape[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_td_samples = np.array([depad(recon_td_samples[i,:,:,:]) for i in range(recon_td_samples.shape[0])])\n",
    "recon_asd_samples = np.array([depad(recon_asd_samples[i,:,:,:]) for i in range(recon_asd_samples.shape[0])])\n",
    "recon_twin_samples = np.array([depad(recon_twin_samples[i,:,:,:]) for i in range(recon_twin_samples.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "results['Z_mu'] = Z_mu\n",
    "results['Z_sigma'] = Z_sigma\n",
    "results['Z'] = Z\n",
    "results['S_mu'] = S_mu\n",
    "results['S_sigma'] = S_sigma\n",
    "results['S'] = S\n",
    "results['Z_sample100'] = Z_sample100\n",
    "results['S_sample100'] = S_sample100\n",
    "results['recon_td_mu'] = recon_td_mu\n",
    "results['recon_asd_mu'] = recon_asd_mu\n",
    "results['recon_twin_mu'] = recon_twin_mu\n",
    "results['recon_td_samples'] = recon_td_samples\n",
    "results['recon_asd_samples'] = recon_asd_samples\n",
    "results['recon_twin_samples'] = recon_twin_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the files\n",
    "[np.savez_compressed(os.path.join(save_dir,key+'.npz'),data=results[key]) for key in list(results.keys())];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump as a pickle just in case\n",
    "import pickle\n",
    "with open((os.path.join(save_dir,'results.pickle')),'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Save as a big numpy arr\n",
    "np.savez_compressed(file=os.path.join(save_dir,'results.npz'),\n",
    "                    Z_mu=Z_mu,\n",
    "                    Z_sigma=Z_sigma,\n",
    "                    Z=Z,\n",
    "                    S_mu=S_mu,\n",
    "                    S_sigma=S_sigma,\n",
    "                    S=S,\n",
    "                    Z_sample100=Z_sample100,\n",
    "                    S_sample100=S_sample100,\n",
    "                    recon_td_mu=recon_td_mu,\n",
    "                    recon_asd_mu=recon_asd_mu,\n",
    "                    recon_twin_mu=recon_twin_mu,\n",
    "                    recon_td_samples=recon_td_samples,\n",
    "                    recon_asd_samples=recon_asd_samples,\n",
    "                    recon_twin_samples=recon_twin_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers = {\n",
    "'latent_dim1' : [latent_dim1],\n",
    "'latent_dim2' : [latent_dim2],\n",
    "'beta' :  [beta],\n",
    "'gamma' :  [gamma],\n",
    "'batch_size' : [batch_size],\n",
    "'kernel_size' : [kernel_size],\n",
    "'filters' : [filters],\n",
    "'intermediate_dim' : [intermediate_dim],\n",
    "'nlayers' : [nlayers],\n",
    "'strides' : [strides],\n",
    "'learning_rate' : [learning_rate],\n",
    "'opt' : str(opt),\n",
    "'loss' : loss[-1],\n",
    "'sigmas' : sigmas[-1],\n",
    "'mus' : mus[-1],\n",
    "'c_sim' : c_sim[-1],\n",
    "'loss_mse' : loss_mse[-1],\n",
    "'loss_kl' : loss_kl[-1],\n",
    "'loss_dc' : loss_dc[-1],\n",
    "'loss_tc' : loss_tc[-1],\n",
    "'nepochs' : epoch}\n",
    "\n",
    "hypers_csv = pd.DataFrame(hypers,index=[0])\n",
    "hypers_csv.to_csv(os.path.join(save_dir,'hyperparams.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae.summary(line_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# shutil.copyfile(src=nb_name,dst=os.path.join(save_dir,nb_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mmfs1/data/aglinska/BC-fMRI-AE/Notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 20 06:56:53 EDT 2022\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEGIN GPU CHECKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gpu_id = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=str(gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:            187          28         157           0           1         157\n",
      "Swap:            11           8           3\n"
     ]
    }
   ],
   "source": [
    "!free -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 15.78173828125\n",
      "Free memory: 15.3359375\n",
      "Used memory: 0.44580078125\n"
     ]
    }
   ],
   "source": [
    "# CHECK GPU\n",
    "import nvidia_smi\n",
    "try:\n",
    "    nvidia_smi.nvmlInit()\n",
    "\n",
    "    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(gpu_id)\n",
    "    # card id 0 hardcoded here, there is also a call to get all available card ids, so we could iterate\n",
    "\n",
    "    info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "\n",
    "    print(\"Total memory:\", (info.total/1024/1024/1024))\n",
    "    print(\"Free memory:\", (info.free/1024/1024/1024))\n",
    "    print(\"Used memory:\", (info.used/1024/1024/1024))\n",
    "\n",
    "    nvidia_smi.nvmlShutdown()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
      "CPU (s):\n",
      "0.7429254548624158\n",
      "GPU (s):\n",
      "0.07665402721613646\n",
      "GPU speedup over CPU: 9x\n"
     ]
    }
   ],
   "source": [
    "# Run GPU test\n",
    "import tensorflow as tf\n",
    "import timeit,pickle\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "  raise SystemError('GPU device not found')\n",
    "\n",
    "def cpu():\n",
    "    with tf.device('/cpu:0'):\n",
    "        random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
    "        net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
    "    return tf.math.reduce_sum(net_cpu)\n",
    "\n",
    "def gpu():\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
    "        net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
    "    return tf.math.reduce_sum(net_gpu)\n",
    "  \n",
    "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
    "cpu()\n",
    "gpu()\n",
    "\n",
    "# Run the op several times.\n",
    "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
    "      '(batch x height x width x channel). Sum of ten runs.')\n",
    "print('CPU (s):')\n",
    "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
    "print(cpu_time)\n",
    "print('GPU (s):')\n",
    "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
    "print(gpu_time)\n",
    "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END GPU CHECKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 s, sys: 514 ms, total: 10.8 s\n",
      "Wall time: 17.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow' from '/data/aglinska/anaconda3/lib/python3.8/site-packages/tensorflow/__init__.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from umap import UMAP\n",
    "\n",
    "import os\n",
    "from datetime import datetime; now = datetime.now\n",
    "tqdm = partial(tqdm, position=0, leave=True) \n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "import helper_funcs;reload(helper_funcs);from helper_funcs import *\n",
    "del helper_funcs\n",
    "import make_models;reload(make_models);from make_models import *\n",
    "del make_models\n",
    "\n",
    "from IPython import display\n",
    "import sys\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "reload(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "analysis_name = 'test_16_b1_g1_f64'\n",
    "\n",
    "# NB name for copying\n",
    "nb_name = '009-train-CVAE-cmats.ipynb'\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim1 = 8\n",
    "latent_dim2 = 8\n",
    "beta = .001\n",
    "gamma = 1\n",
    "batch_size = 32\n",
    "kernel_size = 3\n",
    "filters = 8\n",
    "intermediate_dim = 128\n",
    "nlayers = 4\n",
    "strides = 2\n",
    "learning_rate=0.001\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate,beta_1=0.9,beta_2=0.999,epsilon=1e-07,amsgrad=False,name='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_16_b1_g1_f64\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join('../Assets/tf_weights',analysis_name)\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "print(analysis_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>subID</th>\n",
       "      <th>diag</th>\n",
       "      <th>DSMIV</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>handedness_cat</th>\n",
       "      <th>handedness_score</th>\n",
       "      <th>fiq</th>\n",
       "      <th>...</th>\n",
       "      <th>srs_mannerisms</th>\n",
       "      <th>sqc_total</th>\n",
       "      <th>aq_total</th>\n",
       "      <th>comorbidity</th>\n",
       "      <th>bmi</th>\n",
       "      <th>vineland_sum</th>\n",
       "      <th>dataset</th>\n",
       "      <th>siteID</th>\n",
       "      <th>sub</th>\n",
       "      <th>dataset_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>50004</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.09</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>113.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABIDE1</td>\n",
       "      <td>26</td>\n",
       "      <td>sub-0050004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>50010</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.20</td>\n",
       "      <td>1</td>\n",
       "      <td>L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABIDE1</td>\n",
       "      <td>26</td>\n",
       "      <td>sub-0050010</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>50011</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.93</td>\n",
       "      <td>1</td>\n",
       "      <td>L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABIDE1</td>\n",
       "      <td>26</td>\n",
       "      <td>sub-0050011</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>50012</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.48</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABIDE1</td>\n",
       "      <td>26</td>\n",
       "      <td>sub-0050012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>50014</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.20</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABIDE1</td>\n",
       "      <td>26</td>\n",
       "      <td>sub-0050014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1553</th>\n",
       "      <td>2050</td>\n",
       "      <td>746</td>\n",
       "      <td>30163</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>81.25</td>\n",
       "      <td>136.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABIDE2</td>\n",
       "      <td>23</td>\n",
       "      <td>sub-30163</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>2051</td>\n",
       "      <td>747</td>\n",
       "      <td>30164</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>115.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABIDE2</td>\n",
       "      <td>23</td>\n",
       "      <td>sub-30164</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1555</th>\n",
       "      <td>2052</td>\n",
       "      <td>748</td>\n",
       "      <td>30165</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>120.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABIDE2</td>\n",
       "      <td>23</td>\n",
       "      <td>sub-30165</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556</th>\n",
       "      <td>2053</td>\n",
       "      <td>749</td>\n",
       "      <td>30166</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tourettes/tics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABIDE2</td>\n",
       "      <td>23</td>\n",
       "      <td>sub-30166</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1557</th>\n",
       "      <td>2054</td>\n",
       "      <td>750</td>\n",
       "      <td>30167</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABIDE2</td>\n",
       "      <td>23</td>\n",
       "      <td>sub-30167</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1558 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1  subID  diag  DSMIV    age  gender  \\\n",
       "0              2             2  50004     1    1.0  19.09       1   \n",
       "1              8             8  50010     1    1.0  35.20       1   \n",
       "2              9             9  50011     1    1.0  16.93       1   \n",
       "3             10            10  50012     1    1.0  21.48       1   \n",
       "4             12            12  50014     1    1.0  14.20       1   \n",
       "...          ...           ...    ...   ...    ...    ...     ...   \n",
       "1553        2050           746  30163     2    NaN   8.00       2   \n",
       "1554        2051           747  30164     2    NaN  10.00       2   \n",
       "1555        2052           748  30165     2    NaN  12.00       2   \n",
       "1556        2053           749  30166     2    NaN  10.00       2   \n",
       "1557        2054           750  30167     2    NaN  14.00       2   \n",
       "\n",
       "     handedness_cat  handedness_score    fiq  ...  srs_mannerisms  sqc_total  \\\n",
       "0                 R               NaN  113.0  ...             NaN        NaN   \n",
       "1                 L               NaN   81.0  ...             NaN        NaN   \n",
       "2                 L               NaN  111.0  ...             NaN        NaN   \n",
       "3                 R               NaN  128.0  ...             NaN        NaN   \n",
       "4                 R               NaN   96.0  ...             NaN        NaN   \n",
       "...             ...               ...    ...  ...             ...        ...   \n",
       "1553            1.0             81.25  136.0  ...             2.0        1.0   \n",
       "1554            1.0            100.00  115.0  ...             4.0        2.0   \n",
       "1555            1.0            100.00  120.0  ...             0.0        NaN   \n",
       "1556            1.0            100.00  112.0  ...             6.0        4.0   \n",
       "1557            1.0            100.00  100.0  ...             3.0        4.0   \n",
       "\n",
       "     aq_total     comorbidity bmi  vineland_sum  dataset  siteID          sub  \\\n",
       "0         NaN             NaN NaN           NaN   ABIDE1      26  sub-0050004   \n",
       "1         NaN             NaN NaN           NaN   ABIDE1      26  sub-0050010   \n",
       "2         NaN             NaN NaN           NaN   ABIDE1      26  sub-0050011   \n",
       "3         NaN             NaN NaN           NaN   ABIDE1      26  sub-0050012   \n",
       "4         NaN             NaN NaN           NaN   ABIDE1      26  sub-0050014   \n",
       "...       ...             ...  ..           ...      ...     ...          ...   \n",
       "1553      NaN            none NaN           NaN   ABIDE2      23    sub-30163   \n",
       "1554      NaN            none NaN           NaN   ABIDE2      23    sub-30164   \n",
       "1555      NaN            none NaN           NaN   ABIDE2      23    sub-30165   \n",
       "1556      NaN  tourettes/tics NaN           NaN   ABIDE2      23    sub-30166   \n",
       "1557      NaN            none NaN           NaN   ABIDE2      23    sub-30167   \n",
       "\n",
       "      dataset_id  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              1  \n",
       "...          ...  \n",
       "1553           2  \n",
       "1554           2  \n",
       "1555           2  \n",
       "1556           2  \n",
       "1557           2  \n",
       "\n",
       "[1558 rows x 35 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Data/df_comb_S1558.csv')\n",
    "\n",
    "unique_values = np.unique(df['dataset'].values)\n",
    "new_vals = np.arange(1,len(unique_values)+1)\n",
    "df['dataset_id'] = [new_vals[val==unique_values][0] for val in df['dataset'].values]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1558, 51, 51)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cmats = np.load('../Data/cmats_r51_S1502.npz')['data']\n",
    "# cmats.shape\n",
    "cmats = np.load('../Data/cmats_S1558.npz')['cmats']\n",
    "cmats = (cmats+1)/2\n",
    "cmats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SET UP DATA LOADER AND MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "class cvae_data_loader():\n",
    "    ''' this is the info'''\n",
    "    def __init__(self,cmats,df,batch_size=32):\n",
    "        \n",
    "        self.df = df\n",
    "        self.cmats = cmats\n",
    "        \n",
    "        self.n = len(df)\n",
    "        self.epoch = -1\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "        self.new_epoch()\n",
    "        self.n_batches = int(np.floor(min((len(self.asd_idxs),len(self.td_idxs)))/self.batch_size))\n",
    "        \n",
    "    def new_epoch(self):\n",
    "        \n",
    "        self.asd_idxs = np.nonzero((self.df['diag'].values==1))[0]\n",
    "        self.td_idxs = np.nonzero((self.df['diag'].values==2))[0]\n",
    "        \n",
    "        self.asd_idxs = np.random.permutation(self.asd_idxs)\n",
    "        self.td_idxs = np.random.permutation(self.td_idxs)\n",
    "        \n",
    "        self.epoch += 1\n",
    "        self.b = 0\n",
    "        \n",
    "        \n",
    "    def get_batch(self):\n",
    "        self.b += 1\n",
    "        \n",
    "        if self.b==self.n_batches:\n",
    "            self.new_epoch()\n",
    "        \n",
    "        self.batch_asd_idx = self.asd_idxs[np.arange(self.b*self.batch_size,self.b*self.batch_size+self.batch_size)]\n",
    "        self.batch_td_idx = self.td_idxs[np.arange(self.b*self.batch_size,self.b*self.batch_size+self.batch_size)]\n",
    "        \n",
    "        self.batch_asd = self.cmats[self.batch_asd_idx,:,:]\n",
    "        self.batch_td = self.cmats[self.batch_td_idx,:,:]\n",
    "        \n",
    "        self.batch_df = self.df.iloc[np.hstack((data_loader.batch_asd_idx,data_loader.batch_td_idx))]\n",
    "        \n",
    "        return self.batch_asd,self.batch_td,self.batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1558, 51, 51)\n",
      "(1558, 35)\n"
     ]
    }
   ],
   "source": [
    "print(cmats.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'699|859'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_asd = (df['diag'].values==1).sum()\n",
    "n_td = (df['diag'].values==2).sum()\n",
    "f'{n_asd}|{n_td}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randperm = np.random.permutation(cmats.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fixed Random Split\n",
    "idx_train = [194,238,1116,1225,148,85,2,395,694,1540,1509,560,929,1065,1081,1010,1103,696,138,1445,920,158,796,203,1546,780,171,1244,1284,616,1287,837,368,241,347,804,637,816,692,576,1260,769,1139,260,713,1372,1554,1240,880,1537,1203,363,985,1198,1550,592,643,469,1289,778,498,988,1208,428,193,960,1342,308,881,782,1143,678,886,531,1384,1074,893,1539,1356,1209,137,175,359,1155,1114,1058,1548,1291,1295,274,1277,206,1112,14,649,683,1419,1258,828,418,36,478,555,612,402,906,557,795,811,383,898,1140,448,1334,617,658,633,767,180,1433,1247,146,664,1390,3,823,1217,732,516,76,1080,398,992,1189,721,770,199,874,123,406,244,1032,82,73,1520,704,204,826,1339,525,1353,83,715,1392,1242,1361,1553,271,1320,1551,404,845,870,1060,904,978,1489,1119,426,451,490,706,1490,1125,335,1115,1535,1215,332,1382,549,1100,1524,605,231,641,602,1455,166,758,226,963,687,440,89,1499,1531,580,1385,295,1067,355,1021,1035,998,1190,776,528,619,228,388,1465,150,631,1025,967,1303,1023,1338,1360,741,1183,790,1070,787,947,1147,925,221,818,774,268,322,757,139,593,53,1036,1423,227,506,434,984,1282,672,897,1121,447,289,298,303,37,1253,50,59,316,1037,1515,599,1409,726,688,29,1248,1434,815,802,1097,1538,1135,723,287,1368,876,168,1415,1476,788,606,946,526,924,34,1347,1137,162,959,671,417,991,23,1464,546,748,103,509,1016,626,1378,997,60,708,362,1039,996,505,604,1383,105,666,1408,670,632,1201,724,192,660,462,875,110,1420,861,836,1218,94,300,182,544,833,744,461,1400,879,794,1269,514,501,485,163,1431,1089,52,375,1526,1222,534,1158,1513,1245,31,690,850,229,768,414,1327,256,25,364,667,642,317,1461,846,972,1328,716,510,618,939,1040,1012,1266,504,106,407,378,1053,349,1102,909,981,901,432,307,596,852,1131,223,70,810,113,219,512,340,825,124,1160,429,1178,466,1426,698,1437,1429,1017,1440,1545,1421,766,351,653,48,101,737,834,763,651,1126,416,252,753,273,47,655,1246,390,1085,1333,272,438,743,213,358,173,1185,1161,345,949,1110,1458,491,760,446,1072,374,1192,454,290,519,1351,657,188,1090,1033,450,1196,269,907,1352,181,756,464,88,1337,1071,1011,813,1105,111,515,283,224,1229,64,1381,1052,1241,1219,855,1510,126,1251,1366,1402,1031,24,591,1432,97,132,4,1086,883,200,1451,329,129,1301,638,1,966,463,979,1487,961,962,508,1424,1202,862,636,284,473,1478,165,1428,1519,435,452,214,492,1170,43,629,339,600,237,820,885,856,1250,1341,292,1129,1317,1280,1493,877,1527,205,423,1073,1154,587,1030,430,1389,740,1034,887,323,1273,976,951,410,1214,389,675,1091,624,131,727,1239,480,1001,586,51,1469,65,502,263,196,1014,1505,319,940,240,1466,933,1316,1268,628,0,1447,513,853,360,1186,1477,377,697,422,117,575,67,1375,532,1364,999,154,1076,1130,1422,1297,169,324,436,1542,218,379,474,235,1470,1500,710,12,1443,325,278,625,147,719,703,1216,1311,401,1488,367,293,95,608,1354,1486,1211,578,793,442,116,1555,172,1313,1309,57,614,413,1412,1460,176,342,807,585,611,8,1044,1475,1234,26,1150,92,1418,1124,33,590,369,1174,242,919,493,1050,1541,1349,121,286,17,1095,216,486,817,1283,66,208,1331,680,848,805,380,930,1195,522,1173,1157,941,128,1207,312,1162,251,937,623,562,1285,899,1468,1399,712,63,459,968,1056,529,41,507,1027,1197,841,954,1371,1329,433,684,905,81,1401,285,994,1452,1446,822,1448,18,1141,730,456,1048,1471,195,530,751,468,245,917,46,765,1057,1237,913,746,39,1256,551,859,397,588,1046,54,1226,1398,249,99,891,1388,248,15,1255,257,1022,970,371,420,1149,779,1454,1235,609,1346,1159,1529,405,646,711,399,21,1479,465,411,475,1133,481,1308,1134,1312,1049,932,28,1395,674,1004,183,1450,87,1262,385,1370,705,1127,990,952,236,572,1367,1377,565,376,1261,1064,677,275,1373,1111,1002,1481,120,1233,656,1522,1087,938,914,888,1410,62,302,613,1175,32,1453,328,1223,867,1138,785,1182,1101,90,1517,258,896,366,915,489,843,1151,840,669,926,993,1324,1267,689,1146,1254,659,1442,281,567,1106,425,1075,348,1180,445,569,156,98,153,728,869,1516,1417,540,239,912,167,1290,186,1536,424,250,1107,460,695,1305,839,253,1275,1380,927,844,1343,1456,297,566,1220,144,1213,439,948,11,1427,903,1508,96,1000,331,109,233,747,1018,1363,412,441,673,570,558,346,663,280,1518,1292,1200,1318,812,784,1257,1544,681,122,1026,890,1108,282,1042,686,1120,1483,718,517,806,622,1503,1321,900,799,1144,1472,581,762,621,682,78,1077,922,1063,511,350,1169,1168,1028,373,1414,1068,1552,301,357,484,691,470,1504,934,571,665,1278,554,849,1507,864,772,1299,685,1512,338,1047,662,533,717,77,1485,1104,134]\n",
    "idx_val = [865,386,577,7,1271,1092,607,838,1462,928,291,545,1096,1006,1204,556,755,437,1041,296,1194,288,104,44,603,1314,803,1061,61,797,1181,356,858,315,842,419,1387,1093,543,234,306,100,568,1005,309,343,1038,344,735,973,444,830,341,321,1350,179,829,878,982,1335,455,759,381,1406,277,559,579,185,135,246,1430,408,118,1407,750,1153,141,1263,1480,1029,157,279,1300,1511,1394,1386,548,1403,1113,1136,1302,471,1496,1094,352,541,857,1051,873,370,453,892,5,798,584,1238,561,1118,535,1069,1323,520,1152,1083,1439,634,791,863,552,989,1128,1533,1528,781,1164,1078,1009,1082,889,160,1359,1264,601,615,965,819,860,895,1543,164,497,1172,630,574,232,496,550,1521,384,20,449,1224,771,1436,1498,564,866,1059,1336,467,1055,1369,1188,1441,38,1449,415,391,91,500,1008,479,1165,1117,647,668,1396,809,86,635,1310,700,1288,1167,243,220,775,1191,305,488,353,980,1003,1495,783,1491,482,149,814,1210,701,1236,270,995,1547,313,1024,1374,337,262,1365,1007,1205,936,333,1228,1020,918,207,1397,916,1459,372,1556,1326,1045,403,1163,679,178,16,93,1142,19,1494,49,35,773,1457,1265,30,136,1054,977,831,84,1109,1473,1123,6,159,1249,431,1514,1199,808,151,396,1274,476,553,115,327,40,1501,956,494,539,320,777,1098,745,950,518,187,210,443,1492,1281]\n",
    "idx_test = [910,1304,361,1043,265,547,821,1534,142,1212,202,731,198,595,1532,177,1362,945,1482,197,832,499,749,72,851,45,868,1502,1259,742,1391,536,1435,709,884,1227,330,209,211,527,259,942,1330,639,503,620,264,458,761,789,1444,736,827,1252,69,140,1393,1193,477,654,983,472,487,222,720,693,835,125,1413,1345,847,1307,1079,1438,392,792,1148,573,133,1176,1184,676,56,582,1530,336,108,1296,276,1156,824,247,1293,958,1206,495,739,354,112,334,1298,969,225,955,1355,266,1506,1088,145,1243,1177,944,957,427,733,752,1425,1171,908,1325,314,1231,22,964,27,523,786,10,1306,215,738,1276,1166,1013,734,987,119,130,152,854,127,400,483,1230,58,1015,800,648,1484,801,652,707,923,1497,597,1066,1358,382,261,521,644,267,161,882,1122,729,754,184,1379,75,764,1187,102,1179,725,1557,537,1463,114,421,986,661,921,170,627,393,143,217,1525,524,457,902,1376,563,894,1062,1279,699,1344,212,230,1315,911,1322,55,299,714,310,189,953,1145,9,871,311,254,1467,1294,201,1405,155,74,79,80,1523,387,935,975,610,304,107,598,68,1549,645,594,589,71,702,640,42,1099,1286,13,255,872,174,1270,538,1221,1132,583,1416,974,1340,326,409,1404,1411,931,542,1019,971,1232,318,722,943,191,365,1474,1319,1348,1084,650,1357,294,394,190,1332,1272]\n",
    "\n",
    "idx_train = np.array(idx_train)\n",
    "idx_val = np.array(idx_val)\n",
    "idx_test = np.array(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45\n",
      "0.33\n",
      "\n",
      "\n",
      "0.48\n",
      "0.32\n",
      "\n",
      "\n",
      "0.42\n",
      "0.31\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#idx_train  = np.arange(0,1000)\n",
    "#idx_val  = np.arange(1000,1279)\n",
    "#idx_test  = np.arange(1279,1558)\n",
    "\n",
    "cmats_train = cmats[idx_train,:,:]\n",
    "df_train = df.iloc[idx_train]\n",
    "patients = df_train['diag'].values==1\n",
    "controls = df_train['diag'].values==2\n",
    "print((df_train['diag'].values==1).mean().round(2))\n",
    "print((~np.isnan(df_train['ados_total'].values)).mean().round(2))\n",
    "print('\\n')\n",
    "\n",
    "cmats_val = cmats[idx_val,:,:]\n",
    "df_val = df.iloc[idx_val]\n",
    "patients_val = df_val['diag'].values==1\n",
    "controls_val = df_val['diag'].values==2\n",
    "\n",
    "print((df_val['diag'].values==1).mean().round(2))\n",
    "print((~np.isnan(df_val['ados_total'].values)).mean().round(2))\n",
    "print('\\n')\n",
    "\n",
    "cmats_test = cmats[idx_test,:,:]\n",
    "df_test = df.iloc[idx_test]\n",
    "patients_test = df_test['diag'].values==1\n",
    "controls_test = df_test['diag'].values==2\n",
    "print((df_test['diag'].values==1).mean().round(2))\n",
    "print((~np.isnan(df_test['ados_total'].values)).mean().round(2))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 51, 51)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test Data Loader\n",
    "#data_loader = cvae_data_loader(cmats=cmats, df=df, batch_size=32)\n",
    "data_loader = cvae_data_loader(cmats=cmats_train, df=df_train, batch_size=32)\n",
    "batch_asd,batch_td,batch_df = data_loader.get_batch()\n",
    "batch_asd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0988079908098034, 1.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(batch_asd.min(),batch_asd.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1000,   51,   51])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_size = np.hstack((len(df_train),batch_asd.shape[1:]))\n",
    "data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvae_dashboard(red='PCA'):        \n",
    "    #prediction = cvae.predict(patient_batch)    \n",
    "    predictions = cvae.predict([patient_batch,control_batch])\n",
    "    sigma = (np.e ** z_encoder.predict(patient_batch)[1]).mean()\n",
    "    sigmas.append(sigma)\n",
    "\n",
    "    mu = z_encoder.predict(patient_batch)[0]\n",
    "    mus.append(np.mean([mu[:,0].std() for i in range(mu.shape[1])]))\n",
    "\n",
    "    prediction = predictions[0]\n",
    "\n",
    "    cmat_actual = np.corrcoef(np.vstack((patient_batch.reshape(patient_batch.shape[0],-1),control_batch.reshape(control_batch.shape[0],-1))))\n",
    "    cmat_pred = np.corrcoef(np.vstack((predictions[0].reshape(predictions[0].shape[0],-1),predictions[1].reshape(predictions[1].shape[0],-1))))\n",
    "    c_sim.append(np.corrcoef(get_triu(cmat_pred),get_triu(cmat_actual))[0,1])\n",
    "\n",
    "\n",
    "    ### PROGRESS PLOTTING\n",
    "    display.clear_output(wait=True);\n",
    "    display.display(plt.gcf());\n",
    "    #Organise figure\n",
    "    ncols = 4;nrows=7\n",
    "    if np.mod(i,5)==0:\n",
    "        plt.close()\n",
    "    plt.subplots(nrows,ncols,figsize=(15,15)); # MAKE THE FIGURE\n",
    "\n",
    "\n",
    "    ##### SUBPLOT 1 & 2 ##### \n",
    "\n",
    "    plt.subplot(nrows,ncols/2,1) # PLOT LOSS\n",
    "    \n",
    "    plot_loss = loss[int(len(loss)*.2)::]\n",
    "    plot_loss_val = val_loss[int(len(loss)*.2)::]\n",
    "    \n",
    "    xs = np.arange(len(plot_loss))+1\n",
    "    m,b = np.polyfit(xs,plot_loss,deg=1)\n",
    "    m_val,b_val = np.polyfit(xs,plot_loss_val,deg=1)\n",
    "    \n",
    "    plt.plot(plot_loss)\n",
    "    plt.plot(plot_loss_val)\n",
    "    plt.plot(xs, m*xs + b)\n",
    "    plt.title(f'Epoch {data_loader.epoch} batch {data_loader.b}/{data_loader.n_batches} | Loss {loss[-1]:.2f},| vLoss {val_loss[-1]:.2f}, beta: {m:.4f}')\n",
    "\n",
    "    ##### SUBPLOT 3 ##### \n",
    "    plt.subplot(nrows,ncols,3) # PLOT LOSS LAST 50\n",
    "    hb = 500\n",
    "    if len(loss)>hb:\n",
    "        plot_loss = loss[-hb::]\n",
    "        plot_loss_val = val_loss[-hb::]\n",
    "        \n",
    "        xs = np.arange(len(plot_loss))\n",
    "        m,b = np.polyfit(xs,plot_loss,deg=1)\n",
    "        m_val,b_val = np.polyfit(xs,plot_loss_val,deg=1)\n",
    "        plt.plot(plot_loss)\n",
    "        plt.plot(plot_loss_val)\n",
    "        plt.plot(xs, m*xs + b)\n",
    "        #plt.title(hist)\n",
    "        plt.title(f'Loss last {hb} it, beta {m:.4f}, vbeta {m_val:.4f}')\n",
    "\n",
    "    ##### SUBPLOT 4 ##### \n",
    "    plt.subplot(nrows,ncols,4)\n",
    "    plt.hist(prediction[0,:,:,0].flatten(),alpha=.5)\n",
    "    plt.hist(patient_batch[0,:,:].flatten(),alpha=.5)\n",
    "    plt.legend(['predicted','actual'])\n",
    "    plt.title('in/out histograms')\n",
    "\n",
    "    ##### SUBPLOT 5 ##### \n",
    "    plt.subplot(nrows,ncols,5) #RSA over time\n",
    "    plt.plot(c_sim)\n",
    "    plt.title(f'in/out RSA: {c_sim[-1].round(2)}')\n",
    "\n",
    "    if len(c_sim)>5: # PLOT LS LINE\n",
    "        xs = np.arange(len(c_sim))+1\n",
    "        m,b = np.polyfit(xs,c_sim,deg=1)\n",
    "        plt.plot(xs, m*xs + b)\n",
    "        plt.title(f'in/out RSA: {c_sim[-1].round(2)}, b={m:.4f}')\n",
    "\n",
    "\n",
    "    ##### SUBPLOT 6 ##### \n",
    "    plt.subplot(nrows,ncols,6)\n",
    "    if len(c_sim)>hb:\n",
    "        #plot_loss = loss[-hb::]\n",
    "        xs = np.arange(len(c_sim[-hb::]))\n",
    "        m,b = np.polyfit(xs,c_sim[-hb::],deg=1)\n",
    "        plt.plot(c_sim[-hb::])\n",
    "        plt.plot(xs, m*xs + b)\n",
    "        #plt.title(hist)\n",
    "        plt.title(f'in/outRSA last {hb} it, b={m:.4f}')\n",
    "\n",
    "    # ##### SUBPLOT 7 ##### \n",
    "    # plt.subplot(nrows,ncols,7)\n",
    "    # lbls = ['age','sex','fiq','dsm','site','ados'];\n",
    "    # xs = np.arange(len(lbls));\n",
    "    # plt.bar(xs,batch_rsas[0:6]);\n",
    "    # plt.xticks(xs,labels=lbls);\n",
    "    # plt.title('S RSA')\n",
    "\n",
    "    # ##### SUBPLOT 8 ##### \n",
    "    # plt.subplot(nrows,ncols,8)\n",
    "    # lbls = ['age','sex','fiq','dsm','site','ados'];\n",
    "    # xs = np.arange(len(lbls));\n",
    "    # plt.bar(xs,batch_rsas[6::]);\n",
    "    # plt.xticks(xs,labels=lbls);\n",
    "    # plt.title('Z RSA')\n",
    "    \n",
    "    plt.subplot(nrows,ncols,17)\n",
    "    #keys = ['dataset','site','age','sex','fiq','ados_total','ados_social','ados_comm','ados_rrb',]\n",
    "    keys = ['dataset_id','siteID','age','gender','fiq','ados_total','ados_social','ados_comm','ados_rrb',]\n",
    "    scales = ['ordinal','ordinal','ratio','ordinal','ratio','ratio','ratio','ratio','ratio',]\n",
    "    rsa_res = np.array([key_rsa_cvae(keys[i],scales[i],cmats_train,df_train) for i in range(len(keys))])\n",
    "    #key_rsa_cvae(key,data_scale,use_cmats,rsa_df):\n",
    "    plt.plot(rsa_res[:,0],'.',markersize=15,alpha=.5)\n",
    "    plt.plot(rsa_res[:,1],'.',markersize=15,alpha=.5)\n",
    "    plt.legend(['Z','S']);\n",
    "    plt.xticks(np.arange(rsa_res.shape[0]),labels=keys,rotation=45);\n",
    "    plt.title('RSA Train')\n",
    "\n",
    "\n",
    "    ##### SUBPLOT 9 ##### \n",
    "    plt.subplot(nrows,ncols,9)\n",
    "    plt.plot(sigmas)\n",
    "    plt.title(f'sigmas | {sigmas[-1]:.4f}')\n",
    "\n",
    "\n",
    "    ##### SUBPLOT 10 ##### \n",
    "    plt.subplot(nrows,ncols,10)\n",
    "    plt.plot(mus)\n",
    "    plt.title(f'Mu variance {mus[-1]:.4f}')\n",
    "\n",
    "\n",
    "    ##### SUBPLOT 11 ##### \n",
    "    plt.subplot(nrows,ncols,11)\n",
    "    sns.heatmap(cmat_actual,xticklabels=[],yticklabels=[])\n",
    "    plt.title('input RSA')\n",
    "\n",
    "    ##### SUBPLOT 12 ##### \n",
    "    plt.subplot(nrows,ncols,12)\n",
    "    sns.heatmap(cmat_pred,xticklabels=[],yticklabels=[])\n",
    "    plt.title('output RSA')\n",
    "\n",
    "    # #############################################\n",
    "    # ###################Reconstructions###########\n",
    "    # #############################################\n",
    "\n",
    "    ##### SUBPLOT 13 #####     \n",
    "    rand_sub = np.random.randint(low=0,high=patient_batch.shape[0])\n",
    "\n",
    "    plt.subplot(nrows,ncols,13)\n",
    "    sns.heatmap(patient_batch[rand_sub,:,:])\n",
    "    plt.xticks([]);plt.yticks([]);plt.title('actual')\n",
    "    ##### SUBPLOT 14 #####     \n",
    "    plt.subplot(nrows,ncols,14)\n",
    "    sns.heatmap(prediction[rand_sub,:,:,0])\n",
    "    plt.xticks([]);plt.yticks([]);plt.title('predicted')\n",
    "    # ##### SUBPLOT 15 #####     \n",
    "    plt.subplot(nrows,ncols,15)\n",
    "    sns.heatmap(abs(patient_batch[rand_sub,:,:]-prediction[rand_sub,:,:,0]))\n",
    "    plt.xticks([]);plt.yticks([]);plt.title('difference')\n",
    "\n",
    "\n",
    "    # ##### SUBPLOT 16 #####                                             \n",
    "    # plt.subplot(nrows,ncols,16)\n",
    "    # plt.imshow(np.rot90(prediction[rand_sub,16,:,:,rand_map]))\n",
    "    # plt.xticks([]);plt.yticks([]);plt.title('predicted')\n",
    "    # ##### SUBPLOT 17 #####     \n",
    "    plt.subplot(nrows,ncols,18)\n",
    "    keys = ['dataset_id','siteID','age','gender','fiq','ados_total','ados_social','ados_comm','ados_rrb',]\n",
    "    scales = ['ordinal','ordinal','ratio','ordinal','ratio','ratio','ratio','ratio','ratio',]\n",
    "    rsa_res = np.array([key_rsa_cvae(keys[i],scales[i],cmats_val,df_val) for i in range(len(keys))])\n",
    "    plt.plot(rsa_res[:,0],'.',markersize=15,alpha=.5)\n",
    "    plt.plot(rsa_res[:,1],'.',markersize=15,alpha=.5)\n",
    "    plt.legend(['Z','S']);\n",
    "    plt.xticks(np.arange(rsa_res.shape[0]),labels=keys,rotation=45);\n",
    "    plt.title('RSA VAL')\n",
    "    \n",
    "    # ##### SUBPLOT 18 #####     \n",
    "    plt.subplot(nrows,ncols,19)\n",
    "    keys = ['dataset_id','siteID','age','gender','fiq','ados_total','ados_social','ados_comm','ados_rrb',]\n",
    "    scales = ['ordinal','ordinal','ratio','ordinal','ratio','ratio','ratio','ratio','ratio',]\n",
    "    rsa_res = np.array([key_rsa_cvae(keys[i],scales[i],cmats_test,df_test) for i in range(len(keys))])\n",
    "    plt.plot(rsa_res[:,0],'.',markersize=15,alpha=.5)\n",
    "    plt.plot(rsa_res[:,1],'.',markersize=15,alpha=.5)\n",
    "    plt.legend(['Z','S']);\n",
    "    plt.xticks(np.arange(rsa_res.shape[0]),labels=keys,rotation=45);\n",
    "    plt.title('RSA TEST')\n",
    "    \n",
    "    # ##### SUBPLOT 19 #####     \n",
    "    # plt.subplot(nrows,ncols,19)\n",
    "    # plt.imshow(patient_batch[rand_sub,:,:,16,rand_map])\n",
    "    # plt.xticks([]);plt.yticks([]);plt.title('actual')\n",
    "    # ##### SUBPLOT 20 #####     \n",
    "    # plt.subplot(nrows,ncols,20)\n",
    "    # plt.imshow(prediction[rand_sub,:,:,16,rand_map])\n",
    "    # plt.xticks([]);plt.yticks([]);plt.title('predicted')\n",
    "\n",
    "\n",
    "    # #############################################\n",
    "    # ################### LOSSES ##################\n",
    "    # #############################################\n",
    "\n",
    "\n",
    "    # Validation loss\n",
    "    predictions = cvae.predict([patient_batch_val,control_batch_val])\n",
    "    input_shape = data_size[1:]\n",
    "    reconstruction_loss = tf.keras.losses.mse(K.flatten(patient_batch_val), K.flatten(predictions[0])) \n",
    "    reconstruction_loss += tf.keras.losses.mse(K.flatten(control_batch_val), K.flatten(predictions[1])) \n",
    "    reconstruction_loss *= input_shape[0] * input_shape[1]\n",
    "    val_mse.append(reconstruction_loss)\n",
    "    \n",
    "    \n",
    "    predictions = cvae.predict([patient_batch,control_batch])\n",
    "    input_shape = data_size[1:]\n",
    "    reconstruction_loss = tf.keras.losses.mse(K.flatten(patient_batch), K.flatten(predictions[0])) \n",
    "    reconstruction_loss += tf.keras.losses.mse(K.flatten(control_batch), K.flatten(predictions[1])) \n",
    "    reconstruction_loss *= input_shape[0] * input_shape[1]\n",
    "\n",
    "\n",
    "    tg_z_mean, tg_z_log_var, tg_z = z_encoder.predict(patient_batch)\n",
    "    tg_s_mean, tg_s_log_var, tg_s = s_encoder.predict(patient_batch)\n",
    "\n",
    "    bg_z_mean, bg_z_log_var, bg_z = z_encoder.predict(control_batch)\n",
    "\n",
    "    kl_loss1 = 1 + tg_z_log_var - tf.keras.backend.square(tg_z_mean) - tf.keras.backend.exp(tg_z_log_var)\n",
    "    kl_loss2 = 1 + tg_s_log_var - tf.keras.backend.square(tg_s_mean) - tf.keras.backend.exp(tg_s_log_var)\n",
    "    kl_loss3 = 1 + bg_z_log_var - tf.keras.backend.square(bg_z_mean) - tf.keras.backend.exp(bg_z_log_var)\n",
    "\n",
    "    kl_loss1 = tf.keras.backend.sum(kl_loss1, axis=-1)\n",
    "    kl_loss2 = tf.keras.backend.sum(kl_loss2, axis=-1)\n",
    "    kl_loss3 = tf.keras.backend.sum(kl_loss3, axis=-1)\n",
    "    kl_loss = kl_loss1+kl_loss2+kl_loss3\n",
    "    kl_loss *= -0.5\n",
    "\n",
    "\n",
    "    discriminator = Dense(1, activation='sigmoid')\n",
    "    z1 = Lambda(lambda x: x[:int(batch_size/2),:])(tg_z)\n",
    "    z2 = Lambda(lambda x: x[int(batch_size/2):,:])(tg_z)\n",
    "    s1 = Lambda(lambda x: x[:int(batch_size/2),:])(tg_s)\n",
    "    s2 = Lambda(lambda x: x[int(batch_size/2):,:])(tg_s)\n",
    "\n",
    "    q_bar = tf.keras.layers.concatenate(\n",
    "      [tf.keras.layers.concatenate([s1, z2], axis=1),\n",
    "      tf.keras.layers.concatenate([s2, z1], axis=1)],\n",
    "      axis=0)\n",
    "\n",
    "    q = tf.keras.layers.concatenate(\n",
    "      [tf.keras.layers.concatenate([s1, z1], axis=1),\n",
    "      tf.keras.layers.concatenate([s2, z2], axis=1)],\n",
    "      axis=0)\n",
    "\n",
    "    q_bar_score = (discriminator(q_bar)+.1) *.85 # +.1 * .85 so that it's 0<x<1\n",
    "    q_score = (discriminator(q)+.1) *.85 \n",
    "    tc_loss = K.log(q_score / (1 - q_score)) \n",
    "    discriminator_loss = - K.log(q_score) - K.log(1 - q_bar_score)\n",
    "    discriminator_loss\n",
    "\n",
    "    loss_mse.append(reconstruction_loss.numpy())\n",
    "    loss_kl.append(kl_loss.numpy().mean())\n",
    "    loss_dc.append(tc_loss.numpy().mean())\n",
    "    loss_tc.append(discriminator_loss.numpy().mean())\n",
    "\n",
    "\n",
    "    plt.subplot(nrows,ncols,21) # MSE \n",
    "    plt.plot(loss_mse[int(len(loss_mse)*.2)::])\n",
    "    plt.plot(val_mse[int(len(loss_mse)*.2)::])\n",
    "    plt.title(f'MSE | {loss_mse[-1]:.4f}')\n",
    "\n",
    "    plt.subplot(nrows,ncols,22) # KL loss\n",
    "    plt.plot(loss_kl)\n",
    "    plt.title(f'KL | {loss_kl[-1]:.4f}')    \n",
    "\n",
    "\n",
    "    plt.subplot(nrows,ncols,23) # TC     \n",
    "    plt.plot(loss_tc)\n",
    "    plt.title(f'Total Correlation loss | {loss_tc[-1]:.4f}')    \n",
    "\n",
    "\n",
    "    plt.subplot(nrows,ncols,24) # Disc         \n",
    "    plt.plot(loss_dc)\n",
    "    plt.title(f'discriminator_loss | {loss_dc[-1]:.4f}')    \n",
    "\n",
    "\n",
    "    tg_s = s_encoder.predict(patient_batch)\n",
    "    tg_z = z_encoder.predict(patient_batch)\n",
    "    bg_z = z_encoder.predict(control_batch)\n",
    "\n",
    "    plt.subplot(nrows,ncols,25)\n",
    "    plt.hist(tg_s[2].flatten(),alpha=.5);\n",
    "    plt.hist(tg_z[2].flatten(),alpha=.5);\n",
    "    plt.hist(bg_z[2].flatten(),alpha=.5);\n",
    "    plt.legend(['tg_s','tg_z','bg_z'])\n",
    "    plt.title('Z')\n",
    "\n",
    "\n",
    "    plt.subplot(nrows,ncols,26)\n",
    "    plt.hist(tg_s[0].flatten(),alpha=.5);\n",
    "    plt.hist(tg_z[0].flatten(),alpha=.5);\n",
    "    plt.hist(bg_z[0].flatten(),alpha=.5);\n",
    "    plt.legend(['tg_s','tg_z','bg_z'])\n",
    "    plt.title('Mus')\n",
    "\n",
    "\n",
    "    plt.subplot(nrows,ncols,27)\n",
    "    plt.hist(tg_s[1].flatten(),alpha=.5);\n",
    "    plt.hist(tg_z[1].flatten(),alpha=.5);\n",
    "    plt.hist(bg_z[1].flatten(),alpha=.5);\n",
    "    plt.legend(['tg_s','tg_z','bg_z'])\n",
    "    plt.title('Sigmas')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_rsa_cvae(key,data_scale,use_cmats,rsa_df):\n",
    "    \n",
    "    Z = z_encoder.predict(pad2d(use_cmats[:,:,:,np.newaxis]))[0]\n",
    "    S = s_encoder.predict(pad2d(use_cmats[:,:,:,np.newaxis]))[0]\n",
    "    #rsa_df = use_df.copy()\n",
    "\n",
    "    patients = rsa_df['diag'].values==1\n",
    "    Z = Z[patients,:]\n",
    "    S = S[patients,:]\n",
    "    rsa_df = rsa_df.iloc[patients]\n",
    "    \n",
    "    vec = rsa_df[key].values\n",
    "    e = np.isnan(vec)\n",
    "    z_fit = fit_rsa(make_RDM(vec[~e],data_scale=data_scale, metric='euclidean'),make_RDM(Z[~e],data_scale='ratio', metric='euclidean'))\n",
    "    s_fit = fit_rsa(make_RDM(vec[~e],data_scale=data_scale, metric='euclidean'),make_RDM(S[~e],data_scale='ratio', metric='euclidean'))\n",
    "    \n",
    "    return (z_fit,s_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import mse\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_fMRI_CVAE_3D(input_shape=(51,51,1),\n",
    "                     latent_dim=[2,2],\n",
    "                     beta=1,\n",
    "                     disentangle=False,\n",
    "                     gamma=1,\n",
    "                     bias=True,\n",
    "                     batch_size = 32,\n",
    "                     kernel_size = 3,\n",
    "                     filters = 16,\n",
    "                     intermediate_dim = 128,\n",
    "                     nlayers = 2,\n",
    "                     strides = 2,\n",
    "                     learning_rate=0.001,\n",
    "                     opt=None):\n",
    "    \n",
    "    \n",
    "    ndim_bg = latent_dim[0]\n",
    "    ndim_sl = latent_dim[1]\n",
    "    \n",
    "    image_size, _, channels = input_shape\n",
    "\n",
    "    kernel_regularizer=regularizers.l2(.0001)\n",
    "\n",
    "    # build encoder model\n",
    "    tg_inputs = Input(shape=input_shape, name='tg_inputs')\n",
    "    bg_inputs = Input(shape=input_shape, name='bg_inputs')\n",
    "    \n",
    "    BatchNorm = tf.keras.layers.BatchNormalization(\n",
    "    axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,\n",
    "    beta_initializer='zeros', gamma_initializer='ones',\n",
    "    moving_mean_initializer='zeros',\n",
    "    moving_variance_initializer='ones', beta_regularizer=None,\n",
    "    gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "\n",
    "    #kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0,stddev=5)\n",
    "    kernel_initializer = tf.keras.initializers.RandomUniform()\n",
    "\n",
    "    # generate latent vector Q(z|X)\n",
    "    \n",
    "    \n",
    "    z_h_layer = Dense(intermediate_dim,activation='relu', use_bias=bias,kernel_regularizer=kernel_regularizer)\n",
    "    z_mean_layer = Dense(ndim_bg, name='z_mean', use_bias=bias,kernel_regularizer=kernel_regularizer)\n",
    "    z_log_var_layer = Dense(ndim_bg, name='z_log_var', use_bias=bias,kernel_regularizer=kernel_regularizer)\n",
    "    z_layer = Lambda(sampling, output_shape=(ndim_bg,), name='z')\n",
    "\n",
    "    def z_encoder_func(inputs):\n",
    "        z_h = inputs\n",
    "\n",
    "        these_filters = filters\n",
    "        for i in range(nlayers):\n",
    "            these_filters *= 2\n",
    "            #print(these_filters)\n",
    "            z_h = Conv2D(filters=these_filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    activation='relu',\n",
    "                    strides=strides,\n",
    "                    padding='same',\n",
    "                    use_bias=bias,\n",
    "                    kernel_regularizer=kernel_regularizer)(z_h)\n",
    "        \n",
    "        # shape info needed to build decoder model\n",
    "        shape = K.int_shape(z_h)\n",
    "        z_h = Flatten()(z_h)\n",
    "        z_h = z_h_layer(z_h)\n",
    "        z_mean =  z_mean_layer(z_h)\n",
    "        #z_mean = BatchNorm(z_mean)\n",
    "        \n",
    "        z_log_var =  z_log_var_layer(z_h)\n",
    "        z = z_layer([z_mean, z_log_var])\n",
    "        return z_mean, z_log_var, z, shape\n",
    "\n",
    "    tg_z_mean, tg_z_log_var, tg_z, shape_z = z_encoder_func(tg_inputs)\n",
    "\n",
    "    # generate latent vector Q(z|X)\n",
    "    s_h_layer = Dense(intermediate_dim, activation='relu', use_bias=bias,kernel_regularizer=kernel_regularizer)\n",
    "    s_mean_layer = Dense(ndim_sl, name='s_mean', use_bias=bias,kernel_regularizer=kernel_regularizer)\n",
    "    s_log_var_layer = Dense(ndim_sl, name='s_log_var', use_bias=bias,kernel_regularizer=kernel_regularizer)\n",
    "    s_layer = Lambda(sampling, output_shape=(ndim_sl,), name='s')\n",
    "\n",
    "    def s_encoder_func(inputs):\n",
    "        s_h = inputs\n",
    "        these_filters = filters\n",
    "        for i in range(nlayers):\n",
    "            these_filters *= 2\n",
    "            s_h = Conv2D(filters=these_filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    activation='relu',\n",
    "                    strides=strides,\n",
    "                    use_bias=bias,\n",
    "                    kernel_regularizer=kernel_regularizer,\n",
    "                    padding='same')(s_h)\n",
    "        \n",
    "        # shape info needed to build decoder model\n",
    "        shape = K.int_shape(s_h)\n",
    "        s_h = Flatten()(s_h)\n",
    "        s_h = s_h_layer(s_h)\n",
    "        s_mean =  s_mean_layer(s_h)\n",
    "        #s_mean = BatchNorm(s_mean)\n",
    "        \n",
    "        s_log_var =  s_log_var_layer(s_h)        \n",
    "        s = s_layer([s_mean, s_log_var])\n",
    "        \n",
    "        return s_mean, s_log_var, s, shape\n",
    "\n",
    "    tg_s_mean, tg_s_log_var, tg_s, shape_s = s_encoder_func(tg_inputs)\n",
    "    bg_z_mean, bg_z_log_var, bg_z, _ = z_encoder_func(bg_inputs) # Aidas and Stefano team hax\n",
    "    \n",
    "    \n",
    "    # instantiate encoder models\n",
    "    z_encoder = tf.keras.models.Model(tg_inputs, [tg_z_mean, tg_z_log_var, tg_z], name='z_encoder')\n",
    "    s_encoder = tf.keras.models.Model(tg_inputs, [tg_s_mean, tg_s_log_var, tg_s], name='s_encoder')\n",
    "\n",
    "\n",
    "    # build decoder model\n",
    "    latent_inputs = Input(shape=(ndim_bg+ndim_sl,), name='z_sampling')\n",
    "\n",
    "    x = Dense(intermediate_dim, activation='relu', use_bias=bias,kernel_regularizer=kernel_regularizer,kernel_initializer=kernel_initializer)(latent_inputs)\n",
    "    x = Dense(shape_z[1] * shape_z[2] * shape_z[3], activation='relu', use_bias=bias,kernel_regularizer=kernel_regularizer,kernel_initializer=kernel_initializer)(x)\n",
    "    x = Reshape((shape_z[1], shape_z[2], shape_z[3]))(x)\n",
    "\n",
    "    these_filters = filters*(2**nlayers)/2\n",
    "    for i in range(nlayers-1):\n",
    "        x = Conv2DTranspose(filters=these_filters,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='relu',\n",
    "                          strides=strides,\n",
    "                          use_bias=bias,\n",
    "                          kernel_regularizer=kernel_regularizer,\n",
    "                          padding='same')(x)\n",
    "        these_filters //= 2\n",
    "\n",
    "    outputs = Conv2DTranspose(filters=channels,\n",
    "                            kernel_size=kernel_size,\n",
    "                            activation='sigmoid',\n",
    "                            padding='same',\n",
    "                            strides=strides,\n",
    "                            use_bias=bias,\n",
    "                            kernel_regularizer=kernel_regularizer,\n",
    "                            name='decoder_output')(x)\n",
    "\n",
    "    # instantiate decoder model\n",
    "    cvae_decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "      # decoder.summary()\n",
    "\n",
    "    def zeros_like(x):\n",
    "        return tf.zeros_like(x)\n",
    "\n",
    "    tg_outputs = cvae_decoder(tf.keras.layers.concatenate([tg_z, tg_s], -1))\n",
    "    zeros = tf.keras.layers.Lambda(zeros_like)(tg_s)\n",
    "\n",
    "    bg_outputs = cvae_decoder(tf.keras.layers.concatenate([bg_z, zeros], -1)) # Aidas look into this, is this correct\n",
    "\n",
    "    cvae = tf.keras.models.Model(inputs=[tg_inputs, bg_inputs], \n",
    "                                  outputs=[tg_outputs, bg_outputs],\n",
    "                                  name='contrastive_vae')\n",
    "\n",
    "#     cvae_fg = tf.keras.models.Model(inputs=tg_inputs, \n",
    "#                                   outputs=fg_outputs, \n",
    "#                                   name='contrastive_vae_fg')\n",
    "\n",
    "    if disentangle:\n",
    "        discriminator = Dense(1, activation='sigmoid')\n",
    "\n",
    "        z1 = Lambda(lambda x: x[:int(batch_size/2),:])(tg_z)\n",
    "        z2 = Lambda(lambda x: x[int(batch_size/2):,:])(tg_z)\n",
    "        s1 = Lambda(lambda x: x[:int(batch_size/2),:])(tg_s)\n",
    "        s2 = Lambda(lambda x: x[int(batch_size/2):,:])(tg_s)\n",
    "\n",
    "        q_bar = tf.keras.layers.concatenate(\n",
    "          [tf.keras.layers.concatenate([s1, z2], axis=1),\n",
    "          tf.keras.layers.concatenate([s2, z1], axis=1)],\n",
    "          axis=0)\n",
    "\n",
    "        q = tf.keras.layers.concatenate(\n",
    "          [tf.keras.layers.concatenate([s1, z1], axis=1),\n",
    "          tf.keras.layers.concatenate([s2, z2], axis=1)],\n",
    "          axis=0)\n",
    "\n",
    "        q_bar_score = (discriminator(q_bar)+.1) *.85 # +.1 * .85 so that it's 0<x<1\n",
    "        q_score = (discriminator(q)+.1) *.85 \n",
    "        tc_loss = K.log(q_score / (1 - q_score)) \n",
    "        discriminator_loss = - K.log(q_score) - K.log(1 - q_bar_score)\n",
    "    else:\n",
    "        tc_loss = 0\n",
    "        discriminator_loss = 0\n",
    "\n",
    "\n",
    "    reconstruction_loss = tf.keras.losses.mse(K.flatten(tg_inputs), K.flatten(tg_outputs)) \n",
    "    reconstruction_loss += tf.keras.losses.mse(K.flatten(bg_inputs), K.flatten(bg_outputs)) \n",
    "    reconstruction_loss *= input_shape[0] * input_shape[1] * input_shape[2]\n",
    "\n",
    "    kl_loss1 = 1 + tg_z_log_var - tf.keras.backend.square(tg_z_mean) - tf.keras.backend.exp(tg_z_log_var)\n",
    "    kl_loss2 = 1 + tg_s_log_var - tf.keras.backend.square(tg_s_mean) - tf.keras.backend.exp(tg_s_log_var)\n",
    "    kl_loss3 = 1 + bg_z_log_var - tf.keras.backend.square(bg_z_mean) - tf.keras.backend.exp(bg_z_log_var)\n",
    "\n",
    "    kl_loss1 = tf.keras.backend.sum(kl_loss1, axis=-1)\n",
    "    kl_loss2 = tf.keras.backend.sum(kl_loss2, axis=-1)\n",
    "    kl_loss3 = tf.keras.backend.sum(kl_loss3, axis=-1)\n",
    "\n",
    "    kl_loss = kl_loss1+kl_loss2+kl_loss3\n",
    "    #kl_loss = tf.keras.backend.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    \n",
    "    cvae_loss = tf.keras.backend.mean(reconstruction_loss + beta*kl_loss + gamma*tc_loss + discriminator_loss)\n",
    "    cvae.add_loss(cvae_loss)\n",
    "    \n",
    "    if type(opt)==type(None):\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate,beta_1=0.9,beta_2=0.999,epsilon=1e-07,amsgrad=False,name='Adam')\n",
    "        #opt = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9, momentum=0.9, epsilon=1e-07, centered=False, name='RMSprop')\n",
    "        #opt = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.1, nesterov=False, name='SGD')\n",
    "\n",
    "    cvae.compile(optimizer=opt,run_eagerly=True)\n",
    "    \n",
    "    return cvae, z_encoder, s_encoder, cvae_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# params| 1,183,538\n"
     ]
    }
   ],
   "source": [
    "#import make_models;reload(make_models);from make_models import *\n",
    "batch_size = 32\n",
    "\n",
    "cvae, z_encoder, s_encoder, cvae_decoder = get_fMRI_CVAE_3D(input_shape=(64,64,1),\n",
    "                                                             latent_dim=[latent_dim1,latent_dim2],\n",
    "                                                             beta=beta,\n",
    "                                                             gamma=gamma,\n",
    "                                                             disentangle=True,\n",
    "                                                             bias=True,\n",
    "                                                             batch_size = batch_size,\n",
    "                                                             kernel_size = kernel_size,\n",
    "                                                             filters = filters,\n",
    "                                                             intermediate_dim = intermediate_dim,\n",
    "                                                             nlayers = nlayers,\n",
    "                                                             strides = strides,\n",
    "                                                             learning_rate=learning_rate,\n",
    "                                                             opt=opt)\n",
    "\n",
    "num_params = np.sum([np.prod(val.get_shape()) for val in cvae.trainable_weights])\n",
    "print(f'# params| {num_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 51, 51)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "Layer (type)                                                       Output Shape                                                Param #                \n",
      "======================================================================================================================================================\n",
      "z_sampling (InputLayer)                                            [(None, 16)]                                                0                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "dense_2 (Dense)                                                    (None, 128)                                                 2176                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "dense_3 (Dense)                                                    (None, 2048)                                                264192                 \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "reshape (Reshape)                                                  (None, 4, 4, 128)                                           0                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspose)                                 (None, 8, 8, 64)                                            73792                  \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTranspose)                               (None, 16, 16, 32)                                          18464                  \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTranspose)                               (None, 32, 32, 16)                                          4624                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "decoder_output (Conv2DTranspose)                                   (None, 64, 64, 1)                                           145                    \n",
      "======================================================================================================================================================\n",
      "Total params: 363,393\n",
      "Trainable params: 363,393\n",
      "Non-trainable params: 0\n",
      "______________________________________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cvae_decoder.summary(line_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"z_encoder\"\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "Layer (type)                                     Output Shape                     Param #           Connected to                                      \n",
      "======================================================================================================================================================\n",
      "tg_inputs (InputLayer)                           [(None, 64, 64, 1)]              0                                                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)                               (None, 32, 32, 16)               160               tg_inputs[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)                               (None, 16, 16, 32)               4640              conv2d_22[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)                               (None, 8, 8, 64)                 18496             conv2d_23[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)                               (None, 4, 4, 128)                73856             conv2d_24[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "flatten (Flatten)                                (None, 2048)                     0                 conv2d_25[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "dense (Dense)                                    (None, 128)                      262272            flatten[0][0]                                     \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "z_mean (Dense)                                   (None, 8)                        1032              dense[0][0]                                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "z_log_var (Dense)                                (None, 8)                        1032              dense[0][0]                                       \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "z (Lambda)                                       (None, 8)                        0                 z_mean[0][0]                                      \n",
      "                                                                                                    z_log_var[0][0]                                   \n",
      "======================================================================================================================================================\n",
      "Total params: 361,488\n",
      "Trainable params: 361,488\n",
      "Non-trainable params: 0\n",
      "______________________________________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "z_encoder.summary(line_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"s_encoder\"\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "Layer (type)                                     Output Shape                     Param #           Connected to                                      \n",
      "======================================================================================================================================================\n",
      "tg_inputs (InputLayer)                           [(None, 64, 64, 1)]              0                                                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)                               (None, 32, 32, 16)               160               tg_inputs[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)                               (None, 16, 16, 32)               4640              conv2d_26[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)                               (None, 8, 8, 64)                 18496             conv2d_27[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)                               (None, 4, 4, 128)                73856             conv2d_28[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)                              (None, 2048)                     0                 conv2d_29[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "dense_1 (Dense)                                  (None, 128)                      262272            flatten_1[0][0]                                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "s_mean (Dense)                                   (None, 8)                        1032              dense_1[0][0]                                     \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "s_log_var (Dense)                                (None, 8)                        1032              dense_1[0][0]                                     \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "s (Lambda)                                       (None, 8)                        0                 s_mean[0][0]                                      \n",
      "                                                                                                    s_log_var[0][0]                                   \n",
      "======================================================================================================================================================\n",
      "Total params: 361,488\n",
      "Trainable params: 361,488\n",
      "Non-trainable params: 0\n",
      "______________________________________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "s_encoder.summary(line_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,all_rsas,sigmas,mus,c_sim = [],[],[],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mse,loss_kl,loss_dc,loss_tc = [],[],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_mse = [],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer zero_padding2d is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pad2d = tf.keras.layers.ZeroPadding2D(padding=((6,7),(6,7))) #If tuple of 2 tuples of 2 ints: interpreted as ((top_pad, bottom_pad), (left_pad, right_pad))\n",
    "data_loader = cvae_data_loader(cmats=pad2d(cmats_train[:,:,:,np.newaxis])[:,:,:,0].numpy(), df=df_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depad(padded_cmats):\n",
    "    if padded_cmats.ndim==3:\n",
    "        depadded = padded_cmats[:,6:57,6:57]\n",
    "    elif padded_cmats.ndim==4:\n",
    "        depadded = padded_cmats[:,6:57,6:57,:]\n",
    "    else:\n",
    "        raise Exception('dims not implemented, wtf')\n",
    "    return depadded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_batch,control_batch,batch_df = data_loader.get_batch() # Get a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_batch_val = pad2d(cmats_val[patients_val,:,:][0:batch_size,:,:,np.newaxis])\n",
    "control_batch_val = pad2d(cmats_val[controls_val,:,:][0:batch_size,:,:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-82a0574555a1>:29: MatplotlibDeprecationWarning: Passing non-integers as three-element position specification is deprecated since 3.3 and will be removed two minor releases later.\n",
      "  plt.subplot(nrows,ncols/2,1) # PLOT LOSS\n"
     ]
    }
   ],
   "source": [
    "if train==True:\n",
    "    for epoch in tqdm(range(5000+1)):\n",
    "        for i in range(data_loader.n_batches):\n",
    "\n",
    "            patient_batch,control_batch,batch_df = data_loader.get_batch() # Get a batch\n",
    "\n",
    "            hist = cvae.train_on_batch([patient_batch[:,:,:,np.newaxis],control_batch[:,:,:,np.newaxis]]) # pass a batch\n",
    "\n",
    "            assert not np.isnan(hist),'loss is NaN - you f**cked up'  # check nothing crashed\n",
    "            loss.append(hist) # keep track of loss\n",
    "            \n",
    "            # Validation loss\n",
    "            val_loss.append(cvae.test_on_batch([patient_batch_val[:,:,:,np.newaxis],control_batch_val[:,:,:,np.newaxis]]))\n",
    "\n",
    "            if all((i==0,np.mod(epoch,25)==0)):\n",
    "                cvae_dashboard() # plot training progress\n",
    "                cvae.save_weights(os.path.join(save_dir,'cvae_weights')) # SAVE WEIGHTS\n",
    "                np.save(os.path.join(save_dir,'cvae_loss.npy'),np.array(loss)) # Save loss            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_cmat(cmats):\n",
    "    ns = cmats.shape[0]\n",
    "    ni = cmats.shape[-1]\n",
    "    tridx = np.triu_indices(n=ni,k=1)\n",
    "    mat_flat = np.array([cmats[i,:,:][tridx] for i in range(ns)])\n",
    "    return mat_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_rsa(inMat,key,df,model_scale,return_models=False):\n",
    "    vec = df[key].values\n",
    "    e = np.isnan(vec)\n",
    "\n",
    "    model_rdm = make_RDM(vec[~e],data_scale=model_scale, metric='euclidean')\n",
    "    models = []\n",
    "    model_fit = []\n",
    "\n",
    "    # Make models\n",
    "    for i in range(len(inMat)):\n",
    "        mat = inMat[i]['data']\n",
    "        metric = inMat[i]['metric']\n",
    "        data_scale = inMat[i]['data_scale']\n",
    "\n",
    "        if mat.ndim==3:\n",
    "            model_data = np.array([make_RDM(mat[i,~e,:],data_scale=data_scale, metric=metric) for i in range(10)])\n",
    "        elif mat.ndim==2:\n",
    "            model_data = np.array([make_RDM(mat[~e,:],data_scale=data_scale, metric=metric) for i in range(10)])\n",
    "        models.append(model_data)\n",
    "\n",
    "    for m in range(len(models)):\n",
    "        model_fit.append([fit_rsa(models[m][i,:,:],model_rdm,measure='kendall') for i in range(10)])\n",
    "\n",
    "    res = np.array(model_fit).transpose()\n",
    "\n",
    "    if return_models==True:\n",
    "        return res,models\n",
    "    else:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rsa(rdm_data,rdm_model,measure='pearson',cov=None):\n",
    "    from scipy.stats import kendalltau\n",
    "    from pingouin import partial_corr\n",
    "    \n",
    "    if measure=='pearson':\n",
    "        r = np.corrcoef(get_triu(rdm_data),get_triu(rdm_model))[0,1]\n",
    "    elif measure=='kendall':\n",
    "        r = kendalltau(get_triu(rdm_data),get_triu(rdm_model))[0]\n",
    "    elif measure=='partial_kendall':\n",
    "        arr = pd.DataFrame(np.array([get_triu(rdm_data),get_triu(rdm_model),get_triu(cov)]).transpose(),columns=['x','y','cv1'])\n",
    "        r = partial_corr(data=arr, x='x', y='y', covar='cv1')['r'].values[0]\n",
    "            \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nice_bar(key,rsa,ax=None,figsize=None,dpi=None,fontsize=None,fontsize_star=None,fontweight=None,line_width=None,marker_size=None,title=None,report_t=False,do_pairwise_stars=False,do_one_sample_stars=True,lbls=['VAE','BG','SL']):\n",
    "    \n",
    "    import seaborn as sns\n",
    "    from scipy.stats import ttest_1samp\n",
    "    from scipy.stats import ttest_ind as ttest\n",
    "    \n",
    "    pallete = sns.color_palette()\n",
    "    pallete_new = sns.color_palette()\n",
    "    \n",
    "    if not figsize:\n",
    "        figsize = (5,2)\n",
    "    if not dpi:\n",
    "        dpi = 300\n",
    "        \n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(1,1,figsize=figsize,dpi=dpi)\n",
    "\n",
    "    pallete_new[1]=pallete[0]\n",
    "    pallete_new[0]=pallete[1]\n",
    "    pallete_new[0] = tuple(np.array((.5,.5,.5)))\n",
    "\n",
    "    data=rsa[key]\n",
    "    n = data.shape[0]\n",
    "    c = data.shape[1]\n",
    "    x = np.arange(c)\n",
    "    \n",
    "    if not fontsize:\n",
    "        fontsize = 16\n",
    "        \n",
    "    if not fontsize_star:\n",
    "        fontsize_star = 25\n",
    "    if not fontweight:        \n",
    "        fontweight = 'bold'\n",
    "    if not line_width:    \n",
    "        line_width = 2.5\n",
    "    if not marker_size:            \n",
    "        marker_size = .1\n",
    "\n",
    "\n",
    "    for i in range(c):\n",
    "        plot_data = np.zeros(data.shape)\n",
    "        plot_data[:,i] = data[:,i]\n",
    "\n",
    "        xs = np.repeat(i,n)+(np.random.rand(n)-.5)*.25\n",
    "        sc = plt.scatter(xs,data[:,i],c='k',s = marker_size)\n",
    "        b = sns.barplot(data=plot_data,errcolor='r',linewidth=line_width,errwidth=line_width,facecolor=np.hstack((np.array(pallete_new[i]),.3)),edgecolor=np.hstack((np.array(pallete_new[i]),1)))\n",
    "\n",
    "    locs, labels = plt.yticks()  \n",
    "    new_y = locs\n",
    "    new_y = np.linspace(locs[0],locs[-1],6)\n",
    "    plt.yticks(new_y,labels=[f'{yy:.2f}' for yy in new_y],fontsize=fontsize,fontweight=fontweight)\n",
    "    plt.ylabel('model fit (r)',fontsize=fontsize,fontweight=fontweight)\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "            ax.spines[axis].set_linewidth(line_width)\n",
    "\n",
    "    xlbls = lbls.copy()\n",
    "    \n",
    "    plt.xticks(np.arange(len(xlbls)),labels=xlbls,fontsize=fontsize,fontweight=fontweight)\n",
    "    \n",
    "    if do_one_sample_stars:\n",
    "        one_sample = np.array([ttest_1samp(data[:,i],0) for i in range(len(xlbls))])\n",
    "        one_sample_thresh = np.array((1,.05,.001,.0001))\n",
    "        one_sample_stars = np.array(('n.s.','*','**','***'))\n",
    "        xlbls = lbls.copy()\n",
    "        for i in range(1,c):\n",
    "            these_stars = one_sample_stars[max(np.nonzero(one_sample[i,1]<one_sample_thresh)[0])]\n",
    "            xlbls[i] = f'{xlbls[i]}\\n({these_stars})'\n",
    "        plt.xticks(np.arange(len(xlbls)),labels=xlbls,fontsize=fontsize,fontweight=fontweight,horizontalalignment='center',multialignment='center')\n",
    "\n",
    "    pairwise_t = np.zeros((3,3))\n",
    "    pairwise_p = np.zeros((3,3))\n",
    "\n",
    "    pairwise_sample_thresh = np.array((1,.05,.001,.0001))\n",
    "    pairwise_sample_stars = np.array(('n.s.','*','**','***'))\n",
    "\n",
    "    if report_t:\n",
    "        for i in range(c):\n",
    "            for j in range(c):\n",
    "                t,p = ttest(data[:,i],data[:,j])\n",
    "                mnames = lbls.copy()\n",
    "\n",
    "                if p > .001:\n",
    "                    print(f'{key} {mnames[i]} >  {mnames[j]} | t({data.shape[0]-1}) = {t:.2f} p = {p:.2f}')\n",
    "                else:\n",
    "                    print(f'{key} {mnames[i]} >  {mnames[j]} | t({data.shape[0]-1}) = {t:.2f} p $<$ .001')\n",
    "                pairwise_t[i,j] = t\n",
    "                pairwise_p[i,j] = p\n",
    "\n",
    "    comps = [[1,2]]\n",
    "    if do_pairwise_stars:\n",
    "        for comp_idx in range(len(comps)):\n",
    "            this_comp = comps[comp_idx]\n",
    "            sig_idx = max(np.nonzero(pairwise_p[this_comp[0],this_comp[1]]<pairwise_sample_thresh)[0])\n",
    "            max_y = new_y[-1] + comp_idx*.05\n",
    "            xs = np.array(this_comp)\n",
    "            stars = pairwise_sample_stars[sig_idx]\n",
    "            plt.plot(xs,[max_y,max_y],'k',linewidth=line_width)\n",
    "            plt.text(xs.mean(),max_y,stars,fontsize=fontsize_star,horizontalalignment='center',fontweight=fontweight)\n",
    "        \n",
    "    ylim = plt.ylim()\n",
    "    plt.ylim(np.array(ylim)*(1,1.1))\n",
    "    \n",
    "    if not title:\n",
    "        plt.title(key,fontsize=fontsize*1.5,pad=2,fontweight=fontweight)    \n",
    "    else:\n",
    "        plt.title(title,fontsize=fontsize*1.5,pad=2,fontweight=fontweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "Z_train = np.array([z_encoder.predict(pad2d(cmats_train[:,:,:,np.newaxis]))[2] for _ in range(10)])\n",
    "S_train = np.array([s_encoder.predict(pad2d(cmats_train[:,:,:,np.newaxis]))[2] for _ in range(10)])\n",
    "\n",
    "Z_val = np.array([z_encoder.predict(pad2d(cmats_val[:,:,:,np.newaxis]))[2] for _ in range(10)])\n",
    "S_val = np.array([s_encoder.predict(pad2d(cmats_val[:,:,:,np.newaxis]))[2] for _ in range(10)])\n",
    "\n",
    "Z_test = np.array([z_encoder.predict(pad2d(cmats_test[:,:,:,np.newaxis]))[2] for _ in range(10)])\n",
    "S_test = np.array([s_encoder.predict(pad2d(cmats_test[:,:,:,np.newaxis]))[2] for _ in range(10)])\n",
    "\n",
    "\n",
    "recon_train = np.array([flatten_cmat(np.array(depad(cvae_decoder(np.concatenate((Z_train,S_train),axis=2)[i,:,:])[:,:,:,0]))) for i in range(10)])\n",
    "twin_train = np.array([flatten_cmat(np.array(depad(cvae_decoder(np.concatenate((Z_train,np.zeros(Z_train.shape)),axis=2)[i,:,:])[:,:,:,0]))) for i in range(10)])\n",
    "dmat_train = recon_train-twin_train\n",
    "\n",
    "\n",
    "recon_val = np.array([flatten_cmat(np.array(depad(cvae_decoder(np.concatenate((Z_val,S_val),axis=2)[i,:,:])[:,:,:,0]))) for i in range(10)])\n",
    "twin_val = np.array([flatten_cmat(np.array(depad(cvae_decoder(np.concatenate((Z_val,np.zeros(Z_val.shape)),axis=2)[i,:,:])[:,:,:,0]))) for i in range(10)])\n",
    "dmat_val = recon_val-twin_val\n",
    "\n",
    "recon_test = np.array([flatten_cmat(np.array(depad(cvae_decoder(np.concatenate((Z_test,S_test),axis=2)[i,:,:])[:,:,:,0]))) for i in range(10)])\n",
    "twin_test = np.array([flatten_cmat(np.array(depad(cvae_decoder(np.concatenate((Z_test,np.zeros(Z_test.shape)),axis=2)[i,:,:])[:,:,:,0]))) for i in range(10)])\n",
    "dmat_test = recon_test-twin_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAIN\n",
    "v1 = {'label' : 'CMAT', 'data' : np.array([flatten_cmat(cmats_train[patients,:,:]) for i in range(10)]), 'data_scale' : 'ratio', 'metric' : 'euclidean'}\n",
    "v2 = {'label' : 'Recon', 'data' : recon_train[:,patients,:], 'data_scale' : 'ratio', 'metric' : 'euclidean'}\n",
    "v3 = {'label' : 'DMAT', 'data' : dmat_train[:,patients,:], 'data_scale' : 'ratio', 'metric' : 'euclidean'}\n",
    "v4 = {'label' : 'BG', 'data' : Z_train[:,patients,:], 'data_scale' : 'ratio', 'metric' : 'euclidean'}\n",
    "v5 = {'label' : 'SL', 'data' : S_train[:,patients,:], 'data_scale' : 'ratio', 'metric' : 'euclidean'}\n",
    "inMat_train = [v1,v2,v3,v4,v5]\n",
    "\n",
    "\n",
    "rsa_res_train = dict()\n",
    "keys = ['dataset_id', 'siteID','age', 'gender', 'fiq','DSMIV','ados_total', 'ados_social', 'ados_comm', 'ados_rrb']\n",
    "data_scale = ['ordinal', 'ordinal','ratio', 'ordinal', 'ratio','ordinal','ratio', 'ratio', 'ratio', 'ratio']\n",
    "\n",
    "for i in tqdm(range(len(keys))):\n",
    "    rsa_res_train.update({keys[i] : key_rsa(inMat_train,keys[i],df=df_train.iloc[patients],model_scale=data_scale[i])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Val\n",
    "v1 = {'label' : 'CMAT', 'data' : np.array([flatten_cmat(cmats_val[patients_val,:,:]) for i in range(10)]), 'data_scale' : 'ratio', 'metric' : 'euclidean'}\n",
    "v2 = {'label' : 'Recon', 'data' : recon_val[:,patients_val,:], 'data_scale' : 'ratio', 'metric' : 'euclidean'}\n",
    "v3 = {'label' : 'DMAT', 'data' : dmat_val[:,patients_val,:], 'data_scale' : 'ratio', 'metric' : 'euclidean'}\n",
    "v4 = {'label' : 'BG', 'data' : Z_val[:,patients_val,:], 'data_scale' : 'ratio', 'metric' : 'euclidean'}\n",
    "v5 = {'label' : 'SL', 'data' : S_val[:,patients_val,:], 'data_scale' : 'ratio', 'metric' : 'euclidean'}\n",
    "inMat_val = [v1,v2,v3,v4,v5]\n",
    "\n",
    "\n",
    "rsa_res_val = dict()\n",
    "keys = ['dataset_id', 'siteID','age', 'gender', 'fiq','DSMIV','ados_total', 'ados_social', 'ados_comm', 'ados_rrb']\n",
    "data_scale = ['ordinal', 'ordinal','ratio', 'ordinal', 'ratio','ordinal','ratio', 'ratio', 'ratio', 'ratio']\n",
    "\n",
    "for i in tqdm(range(len(keys))):\n",
    "    rsa_res_val.update({keys[i] : key_rsa(inMat_val,keys[i],df=df_val.iloc[patients_val],model_scale=data_scale[i])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test\n",
    "v1 = {'label' : 'CMAT', 'data' : np.array([flatten_cmat(cmats_test[patients_test,:,:]) for i in range(10)]), 'data_scale' : 'ratio', 'metric' : 'euclidean'}\n",
    "v2 = {'label' : 'Recon', 'data' : recon_test[:,patients_test,:], 'data_scale' : 'ratio', 'metric' : 'euclidean'}\n",
    "v3 = {'label' : 'DMAT', 'data' : dmat_test[:,patients_test,:], 'data_scale' : 'ratio', 'metric' : 'euclidean'}\n",
    "v4 = {'label' : 'BG', 'data' : Z_test[:,patients_test,:], 'data_scale' : 'ratio', 'metric' : 'euclidean'}\n",
    "v5 = {'label' : 'SL', 'data' : S_test[:,patients_test,:], 'data_scale' : 'ratio', 'metric' : 'euclidean'}\n",
    "inMat_test = [v1,v2,v3,v4,v5]\n",
    "\n",
    "\n",
    "rsa_res_test = dict()\n",
    "keys = ['dataset_id', 'siteID','age', 'gender', 'fiq','DSMIV','ados_total', 'ados_social', 'ados_comm', 'ados_rrb']\n",
    "data_scale = ['ordinal', 'ordinal','ratio', 'ordinal', 'ratio','ordinal','ratio', 'ratio', 'ratio', 'ratio']\n",
    "\n",
    "for i in tqdm(range(len(keys))):\n",
    "    rsa_res_test.update({keys[i] : key_rsa(inMat_test,keys[i],df=df_test.iloc[patients_test],model_scale=data_scale[i])})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 3\n",
    "nrows = len(keys)\n",
    "plt.figure(figsize=np.array((ncols,nrows))*4)\n",
    "\n",
    "\n",
    "for i,key in enumerate(keys):\n",
    "    \n",
    "    plt.figure(figsize=(15,5))\n",
    "    ax = plt.subplot(1,3,1)\n",
    "    rsa_res = rsa_res_train\n",
    "    inMat = inMat_train\n",
    "    plot_nice_bar(key,rsa_res,ax=ax,figsize=None,dpi=300,fontsize=12,fontsize_star=12,fontweight='bold',line_width=2.5,marker_size=12,title=keys[i]+'\\n train',lbls=[inMat[j]['label'] for j in range(len(inMat))])\n",
    "\n",
    "    ax = plt.subplot(1,3,2)\n",
    "    rsa_res = rsa_res_val\n",
    "    inMat = inMat_val\n",
    "    plot_nice_bar(key,rsa_res,ax=ax,figsize=None,dpi=300,fontsize=12,fontsize_star=12,fontweight='bold',line_width=2.5,marker_size=12,title=keys[i]+'\\n val',lbls=[inMat[j]['label'] for j in range(len(inMat))])\n",
    "\n",
    "    ax = plt.subplot(1,3,3)\n",
    "    rsa_res = rsa_res_test\n",
    "    inMat = inMat_test\n",
    "    plot_nice_bar(key,rsa_res,ax=ax,figsize=None,dpi=300,fontsize=12,fontsize_star=12,fontweight='bold',line_width=2.5,marker_size=12,title=keys[i]+'\\n test',lbls=[inMat[j]['label'] for j in range(len(inMat))])\n",
    "        \n",
    "    plt.subplots_adjust(\n",
    "        left=None,\n",
    "        bottom=None,\n",
    "        right=None,\n",
    "        top=None,\n",
    "        wspace=.5,\n",
    "        hspace=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1-(cmats[~patients,:,:]-recon_td_mu).var()/cmats[~patients,:,:].var()).round(2)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmats[~patients,:,:].var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = (cmats[~patients,:,:]-cmats[~patients,:,:].mean(axis=0)).var()\n",
    "m2 = (cmats[~patients,:,:]-recon_td_mu).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-m2/m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save latent representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvae_predict(z,s,asd):\n",
    "    \n",
    "    assert z.shape[0]==s.shape[0],'bad'\n",
    "    \n",
    "    if np.array(asd).ndim==0:\n",
    "        asd = np.repeat(asd,z.shape[0])\n",
    "        \n",
    "    z_ = np.zeros(s.shape)\n",
    "    s[~asd,:] = 0\n",
    "    \n",
    "    l = np.hstack((z,s))    \n",
    "    recon = cvae_decoder.predict(l)\n",
    "    \n",
    "    return recon[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = df['diag'].values==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmats_padded = pad2d(cmats[:,:,:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_mu,Z_sigma,Z = z_encoder.predict(cmats_padded)\n",
    "S_mu,S_sigma,S = s_encoder.predict(cmats_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_sample100 = np.array([z_encoder.predict(cmats_padded)[2] for _ in tqdm(range(100))])\n",
    "S_sample100 = np.array([s_encoder.predict(cmats_padded)[2] for _ in tqdm(range(100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_td_mu = cvae_predict(Z_mu[~patients,:],S_mu[~patients,:],asd=False)\n",
    "recon_asd_mu = cvae_predict(Z_mu[patients,:],S_mu[patients,:],asd=True)\n",
    "recon_twin_mu = cvae_predict(Z_mu[patients,:],S_mu[patients,:],asd=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_td_mu = depad(recon_td_mu)\n",
    "recon_asd_mu = depad(recon_asd_mu)\n",
    "recon_twin_mu = depad(recon_twin_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_td_samples = np.array([cvae_predict(Z_sample100[i,~patients,:],S_sample100[i,~patients,:],asd=False) for i in tqdm(range(Z_sample100.shape[0]))])\n",
    "recon_asd_samples = np.array([cvae_predict(Z_sample100[i,patients,:],S_sample100[i,patients,:],asd=True) for i in tqdm(range(Z_sample100.shape[0]))])\n",
    "recon_twin_samples = np.array([cvae_predict(Z_sample100[i,patients,:],S_sample100[i,patients,:],asd=False) for i in tqdm(range(Z_sample100.shape[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_td_samples = np.array([depad(recon_td_samples[i,:,:,:]) for i in range(recon_td_samples.shape[0])])\n",
    "recon_asd_samples = np.array([depad(recon_asd_samples[i,:,:,:]) for i in range(recon_asd_samples.shape[0])])\n",
    "recon_twin_samples = np.array([depad(recon_twin_samples[i,:,:,:]) for i in range(recon_twin_samples.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "results['Z_mu'] = Z_mu\n",
    "results['Z_sigma'] = Z_sigma\n",
    "results['Z'] = Z\n",
    "results['S_mu'] = S_mu\n",
    "results['S_sigma'] = S_sigma\n",
    "results['S'] = S\n",
    "results['Z_sample100'] = Z_sample100\n",
    "results['S_sample100'] = S_sample100\n",
    "results['recon_td_mu'] = recon_td_mu\n",
    "results['recon_asd_mu'] = recon_asd_mu\n",
    "results['recon_twin_mu'] = recon_twin_mu\n",
    "results['recon_td_samples'] = recon_td_samples\n",
    "results['recon_asd_samples'] = recon_asd_samples\n",
    "results['recon_twin_samples'] = recon_twin_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the files\n",
    "[np.savez_compressed(os.path.join(save_dir,key+'.npz'),data=results[key]) for key in list(results.keys())];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump as a pickle just in case\n",
    "import pickle\n",
    "with open((os.path.join(save_dir,'results.pickle')),'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Save as a big numpy arr\n",
    "np.savez_compressed(file=os.path.join(save_dir,'results.npz'),\n",
    "                    Z_mu=Z_mu,\n",
    "                    Z_sigma=Z_sigma,\n",
    "                    Z=Z,\n",
    "                    S_mu=S_mu,\n",
    "                    S_sigma=S_sigma,\n",
    "                    S=S,\n",
    "                    Z_sample100=Z_sample100,\n",
    "                    S_sample100=S_sample100,\n",
    "                    recon_td_mu=recon_td_mu,\n",
    "                    recon_asd_mu=recon_asd_mu,\n",
    "                    recon_twin_mu=recon_twin_mu,\n",
    "                    recon_td_samples=recon_td_samples,\n",
    "                    recon_asd_samples=recon_asd_samples,\n",
    "                    recon_twin_samples=recon_twin_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers = {\n",
    "'latent_dim1' : [latent_dim1],\n",
    "'latent_dim2' : [latent_dim2],\n",
    "'beta' :  [beta],\n",
    "'gamma' :  [gamma],\n",
    "'batch_size' : [batch_size],\n",
    "'kernel_size' : [kernel_size],\n",
    "'filters' : [filters],\n",
    "'intermediate_dim' : [intermediate_dim],\n",
    "'nlayers' : [nlayers],\n",
    "'strides' : [strides],\n",
    "'learning_rate' : [learning_rate],\n",
    "'opt' : str(opt),\n",
    "'loss' : loss[-1],\n",
    "'sigmas' : sigmas[-1],\n",
    "'mus' : mus[-1],\n",
    "'c_sim' : c_sim[-1],\n",
    "'loss_mse' : loss_mse[-1],\n",
    "'loss_kl' : loss_kl[-1],\n",
    "'loss_dc' : loss_dc[-1],\n",
    "'loss_tc' : loss_tc[-1],\n",
    "'nepochs' : epoch}\n",
    "\n",
    "hypers_csv = pd.DataFrame(hypers,index=[0])\n",
    "hypers_csv.to_csv(os.path.join(save_dir,'hyperparams.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae.summary(line_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# shutil.copyfile(src=nb_name,dst=os.path.join(save_dir,nb_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

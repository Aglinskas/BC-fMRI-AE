{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e506606f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mmfs1/data/aglinska/BC-fMRI-AE/Notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bda8033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr  1 05:24:11 EDT 2022\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83f62197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 s, sys: 5.06 s, total: 15.7 s\n",
      "Wall time: 32.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow' from '/data/aglinska/anaconda3/lib/python3.8/site-packages/tensorflow/__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from umap import UMAP\n",
    "\n",
    "import os\n",
    "from datetime import datetime; now = datetime.now\n",
    "tqdm = partial(tqdm, position=0, leave=True) \n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "import helper_funcs;reload(helper_funcs);from helper_funcs import *\n",
    "del helper_funcs\n",
    "import make_models;reload(make_models);from make_models import *\n",
    "del make_models\n",
    "\n",
    "from IPython import display\n",
    "import sys\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "import shutil\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "reload(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db73a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVAE_2022-03-25 18:28:49.469238\n",
      "../Assets/tf_weights/CVAE_2022-03-25 18:28:49.469238\n"
     ]
    }
   ],
   "source": [
    "analysis_name = 'CVAE_2022-03-25 18:28:49.469238'\n",
    "save_dir = os.path.join('../Assets/tf_weights',analysis_name)\n",
    "print(analysis_name)\n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93d87b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>diag</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>fiq</th>\n",
       "      <th>site</th>\n",
       "      <th>DSMIV</th>\n",
       "      <th>ados_total</th>\n",
       "      <th>ados_social</th>\n",
       "      <th>ados_comm</th>\n",
       "      <th>ados_rrb</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>50002</td>\n",
       "      <td>1</td>\n",
       "      <td>16.77</td>\n",
       "      <td>1</td>\n",
       "      <td>103.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>50004</td>\n",
       "      <td>1</td>\n",
       "      <td>19.09</td>\n",
       "      <td>1</td>\n",
       "      <td>113.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>50005</td>\n",
       "      <td>1</td>\n",
       "      <td>13.73</td>\n",
       "      <td>2</td>\n",
       "      <td>119.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50006</td>\n",
       "      <td>1</td>\n",
       "      <td>13.37</td>\n",
       "      <td>1</td>\n",
       "      <td>109.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>50011</td>\n",
       "      <td>1</td>\n",
       "      <td>16.93</td>\n",
       "      <td>1</td>\n",
       "      <td>111.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>746</td>\n",
       "      <td>30163</td>\n",
       "      <td>2</td>\n",
       "      <td>8.00</td>\n",
       "      <td>2</td>\n",
       "      <td>136.0</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>747</td>\n",
       "      <td>30164</td>\n",
       "      <td>2</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2</td>\n",
       "      <td>115.0</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>748</td>\n",
       "      <td>30165</td>\n",
       "      <td>2</td>\n",
       "      <td>12.00</td>\n",
       "      <td>2</td>\n",
       "      <td>120.0</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>749</td>\n",
       "      <td>30166</td>\n",
       "      <td>2</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2</td>\n",
       "      <td>112.0</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>750</td>\n",
       "      <td>30167</td>\n",
       "      <td>2</td>\n",
       "      <td>14.00</td>\n",
       "      <td>2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1502 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  participant_id  diag    age  sex    fiq  site  DSMIV  \\\n",
       "0              0           50002     1  16.77    1  103.0     4    1.0   \n",
       "1              2           50004     1  19.09    1  113.0     4    1.0   \n",
       "2              3           50005     1  13.73    2  119.0     4    1.0   \n",
       "3              4           50006     1  13.37    1  109.0     4    1.0   \n",
       "4              9           50011     1  16.93    1  111.0     4    1.0   \n",
       "...          ...             ...   ...    ...  ...    ...   ...    ...   \n",
       "1497         746           30163     2   8.00    2  136.0    29    NaN   \n",
       "1498         747           30164     2  10.00    2  115.0    29    NaN   \n",
       "1499         748           30165     2  12.00    2  120.0    29    NaN   \n",
       "1500         749           30166     2  10.00    2  112.0    29    NaN   \n",
       "1501         750           30167     2  14.00    2  100.0    29    NaN   \n",
       "\n",
       "      ados_total  ados_social  ados_comm  ados_rrb  dataset  \n",
       "0           12.0          8.0        4.0       3.0        1  \n",
       "1           18.0         12.0        6.0       2.0        1  \n",
       "2           12.0          8.0        4.0       1.0        1  \n",
       "3           12.0          8.0        4.0       4.0        1  \n",
       "4           13.0          9.0        4.0       NaN        1  \n",
       "...          ...          ...        ...       ...      ...  \n",
       "1497         NaN          NaN        NaN       NaN        2  \n",
       "1498         NaN          NaN        NaN       NaN        2  \n",
       "1499         NaN          NaN        NaN       NaN        2  \n",
       "1500         NaN          NaN        NaN       NaN        2  \n",
       "1501         NaN          NaN        NaN       NaN        2  \n",
       "\n",
       "[1502 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Data/comb_df.csv')\n",
    "\n",
    "# Fix site id\n",
    "unique_values = np.unique(df['site'].values)\n",
    "new_vals = np.arange(1,len(unique_values)+1)\n",
    "df['site'] = [new_vals[val==unique_values][0] for val in df['site'].values]\n",
    "\n",
    "unique_values = np.unique(df['dataset'].values)\n",
    "new_vals = np.arange(1,len(unique_values)+1)\n",
    "df['dataset'] = [new_vals[val==unique_values][0] for val in df['dataset'].values]\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e94a367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1502, 51, 51)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmats = np.load('../Data/cmats_r51_S1502.npz')['data']\n",
    "cmats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31bd25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "class cvae_data_loader():\n",
    "    ''' this is the info'''\n",
    "    def __init__(self,cmats,df,batch_size=32):\n",
    "        \n",
    "        self.df = df\n",
    "        self.cmats = cmats\n",
    "        \n",
    "        self.n = len(df)\n",
    "        self.epoch = -1\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "        self.new_epoch()\n",
    "        self.n_batches = int(np.floor(min((len(self.asd_idxs),len(self.td_idxs)))/self.batch_size))\n",
    "        \n",
    "    def new_epoch(self):\n",
    "        \n",
    "        self.asd_idxs = np.nonzero((self.df['diag'].values==1))[0]\n",
    "        self.td_idxs = np.nonzero((self.df['diag'].values==2))[0]\n",
    "        \n",
    "        self.asd_idxs = np.random.permutation(self.asd_idxs)\n",
    "        self.td_idxs = np.random.permutation(self.td_idxs)\n",
    "        \n",
    "        self.epoch += 1\n",
    "        self.b = 0\n",
    "        \n",
    "        \n",
    "    def get_batch(self):\n",
    "        self.b += 1\n",
    "        \n",
    "        if self.b==self.n_batches:\n",
    "            self.new_epoch()\n",
    "        \n",
    "        self.batch_asd_idx = self.asd_idxs[np.arange(self.b*self.batch_size,self.b*self.batch_size+self.batch_size)]\n",
    "        self.batch_td_idx = self.td_idxs[np.arange(self.b*self.batch_size,self.b*self.batch_size+self.batch_size)]\n",
    "        \n",
    "        self.batch_asd = self.cmats[self.batch_asd_idx,:,:]\n",
    "        self.batch_td = self.cmats[self.batch_td_idx,:,:]\n",
    "        \n",
    "        self.batch_df = self.df.iloc[np.hstack((data_loader.batch_asd_idx,data_loader.batch_td_idx))]\n",
    "        \n",
    "        return self.batch_asd,self.batch_td,self.batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44981d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 51, 51)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test Data Loader\n",
    "data_loader = cvae_data_loader(cmats=cmats, df=df, batch_size=32)\n",
    "batch_asd,batch_td,batch_df = data_loader.get_batch()\n",
    "batch_asd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ac3fb65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10454959570513123, 1.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(batch_asd.min(),batch_asd.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13a04d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1502,   51,   51])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_size = np.hstack((len(df),batch_asd.shape[1:]))\n",
    "data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53f0993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import mse\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_fMRI_CVAE_3D(input_shape=(51,51,1),\n",
    "                     latent_dim=[2,2],\n",
    "                     beta=1,\n",
    "                     disentangle=False,\n",
    "                     gamma=1,\n",
    "                     bias=True,\n",
    "                     batch_size = 32,\n",
    "                     kernel_size = 3,\n",
    "                     filters = 16,\n",
    "                     intermediate_dim = 128,\n",
    "                     nlayers = 2,\n",
    "                     strides = 2,\n",
    "                     learning_rate=0.001,\n",
    "                     opt=None):\n",
    "    \n",
    "    \n",
    "    ndim_bg = latent_dim[0]\n",
    "    ndim_sl = latent_dim[1]\n",
    "    \n",
    "    image_size, _, channels = input_shape\n",
    "\n",
    "    kernel_regularizer=regularizers.l2(.0001)\n",
    "\n",
    "    # build encoder model\n",
    "    tg_inputs = Input(shape=input_shape, name='tg_inputs')\n",
    "    bg_inputs = Input(shape=input_shape, name='bg_inputs')\n",
    "    \n",
    "    BatchNorm = tf.keras.layers.BatchNormalization(\n",
    "    axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,\n",
    "    beta_initializer='zeros', gamma_initializer='ones',\n",
    "    moving_mean_initializer='zeros',\n",
    "    moving_variance_initializer='ones', beta_regularizer=None,\n",
    "    gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "\n",
    "    #kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0,stddev=5)\n",
    "    kernel_initializer = tf.keras.initializers.RandomUniform()\n",
    "\n",
    "    # generate latent vector Q(z|X)\n",
    "    \n",
    "    \n",
    "    z_h_layer = Dense(intermediate_dim,activation='relu', use_bias=bias,kernel_regularizer=kernel_regularizer)\n",
    "    z_mean_layer = Dense(ndim_bg, name='z_mean', use_bias=bias,kernel_regularizer=kernel_regularizer)\n",
    "    z_log_var_layer = Dense(ndim_bg, name='z_log_var', use_bias=bias,kernel_regularizer=kernel_regularizer)\n",
    "    z_layer = Lambda(sampling, output_shape=(ndim_bg,), name='z')\n",
    "\n",
    "    def z_encoder_func(inputs):\n",
    "        z_h = inputs\n",
    "\n",
    "        these_filters = filters\n",
    "        for i in range(nlayers):\n",
    "            these_filters *= 2\n",
    "            #print(these_filters)\n",
    "            z_h = Conv2D(filters=these_filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    activation='relu',\n",
    "                    strides=strides,\n",
    "                    padding='same',\n",
    "                    use_bias=bias,\n",
    "                    kernel_regularizer=kernel_regularizer)(z_h)\n",
    "        \n",
    "        # shape info needed to build decoder model\n",
    "        shape = K.int_shape(z_h)\n",
    "        z_h = Flatten()(z_h)\n",
    "        z_h = z_h_layer(z_h)\n",
    "        z_mean =  z_mean_layer(z_h)\n",
    "        #z_mean = BatchNorm(z_mean)\n",
    "        \n",
    "        z_log_var =  z_log_var_layer(z_h)\n",
    "        z = z_layer([z_mean, z_log_var])\n",
    "        return z_mean, z_log_var, z, shape\n",
    "\n",
    "    tg_z_mean, tg_z_log_var, tg_z, shape_z = z_encoder_func(tg_inputs)\n",
    "\n",
    "    # generate latent vector Q(z|X)\n",
    "    s_h_layer = Dense(intermediate_dim, activation='relu', use_bias=bias,kernel_regularizer=kernel_regularizer)\n",
    "    s_mean_layer = Dense(ndim_sl, name='s_mean', use_bias=bias,kernel_regularizer=kernel_regularizer)\n",
    "    s_log_var_layer = Dense(ndim_sl, name='s_log_var', use_bias=bias,kernel_regularizer=kernel_regularizer)\n",
    "    s_layer = Lambda(sampling, output_shape=(ndim_sl,), name='s')\n",
    "\n",
    "    def s_encoder_func(inputs):\n",
    "        s_h = inputs\n",
    "        these_filters = filters\n",
    "        for i in range(nlayers):\n",
    "            these_filters *= 2\n",
    "            s_h = Conv2D(filters=these_filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    activation='relu',\n",
    "                    strides=strides,\n",
    "                    use_bias=bias,\n",
    "                    kernel_regularizer=kernel_regularizer,\n",
    "                    padding='same')(s_h)\n",
    "        \n",
    "        # shape info needed to build decoder model\n",
    "        shape = K.int_shape(s_h)\n",
    "        s_h = Flatten()(s_h)\n",
    "        s_h = s_h_layer(s_h)\n",
    "        s_mean =  s_mean_layer(s_h)\n",
    "        #s_mean = BatchNorm(s_mean)\n",
    "        \n",
    "        s_log_var =  s_log_var_layer(s_h)        \n",
    "        s = s_layer([s_mean, s_log_var])\n",
    "        \n",
    "        return s_mean, s_log_var, s, shape\n",
    "\n",
    "    tg_s_mean, tg_s_log_var, tg_s, shape_s = s_encoder_func(tg_inputs)\n",
    "    bg_z_mean, bg_z_log_var, bg_z, _ = z_encoder_func(bg_inputs) # Aidas and Stefano team hax\n",
    "    \n",
    "    \n",
    "    # instantiate encoder models\n",
    "    z_encoder = tf.keras.models.Model(tg_inputs, [tg_z_mean, tg_z_log_var, tg_z], name='z_encoder')\n",
    "    s_encoder = tf.keras.models.Model(tg_inputs, [tg_s_mean, tg_s_log_var, tg_s], name='s_encoder')\n",
    "\n",
    "\n",
    "    # build decoder model\n",
    "    latent_inputs = Input(shape=(ndim_bg+ndim_sl,), name='z_sampling')\n",
    "\n",
    "    x = Dense(intermediate_dim, activation='relu', use_bias=bias,kernel_regularizer=kernel_regularizer,kernel_initializer=kernel_initializer)(latent_inputs)\n",
    "    x = Dense(shape_z[1] * shape_z[2] * shape_z[3], activation='relu', use_bias=bias,kernel_regularizer=kernel_regularizer,kernel_initializer=kernel_initializer)(x)\n",
    "    x = Reshape((shape_z[1], shape_z[2], shape_z[3]))(x)\n",
    "\n",
    "    these_filters = filters*(2**nlayers)/2\n",
    "    for i in range(nlayers-1):\n",
    "        x = Conv2DTranspose(filters=these_filters,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation='relu',\n",
    "                          strides=strides,\n",
    "                          use_bias=bias,\n",
    "                          kernel_regularizer=kernel_regularizer,\n",
    "                          padding='same')(x)\n",
    "        these_filters //= 2\n",
    "\n",
    "    outputs = Conv2DTranspose(filters=channels,\n",
    "                            kernel_size=kernel_size,\n",
    "                            activation='sigmoid',\n",
    "                            padding='same',\n",
    "                            strides=strides,\n",
    "                            use_bias=bias,\n",
    "                            kernel_regularizer=kernel_regularizer,\n",
    "                            name='decoder_output')(x)\n",
    "\n",
    "    # instantiate decoder model\n",
    "    cvae_decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "      # decoder.summary()\n",
    "\n",
    "    def zeros_like(x):\n",
    "        return tf.zeros_like(x)\n",
    "\n",
    "    tg_outputs = cvae_decoder(tf.keras.layers.concatenate([tg_z, tg_s], -1))\n",
    "    zeros = tf.keras.layers.Lambda(zeros_like)(tg_s)\n",
    "\n",
    "    bg_outputs = cvae_decoder(tf.keras.layers.concatenate([bg_z, zeros], -1)) # Aidas look into this, is this correct\n",
    "\n",
    "    cvae = tf.keras.models.Model(inputs=[tg_inputs, bg_inputs], \n",
    "                                  outputs=[tg_outputs, bg_outputs],\n",
    "                                  name='contrastive_vae')\n",
    "\n",
    "#     cvae_fg = tf.keras.models.Model(inputs=tg_inputs, \n",
    "#                                   outputs=fg_outputs, \n",
    "#                                   name='contrastive_vae_fg')\n",
    "\n",
    "    if disentangle:\n",
    "        discriminator = Dense(1, activation='sigmoid')\n",
    "\n",
    "        z1 = Lambda(lambda x: x[:int(batch_size/2),:])(tg_z)\n",
    "        z2 = Lambda(lambda x: x[int(batch_size/2):,:])(tg_z)\n",
    "        s1 = Lambda(lambda x: x[:int(batch_size/2),:])(tg_s)\n",
    "        s2 = Lambda(lambda x: x[int(batch_size/2):,:])(tg_s)\n",
    "\n",
    "        q_bar = tf.keras.layers.concatenate(\n",
    "          [tf.keras.layers.concatenate([s1, z2], axis=1),\n",
    "          tf.keras.layers.concatenate([s2, z1], axis=1)],\n",
    "          axis=0)\n",
    "\n",
    "        q = tf.keras.layers.concatenate(\n",
    "          [tf.keras.layers.concatenate([s1, z1], axis=1),\n",
    "          tf.keras.layers.concatenate([s2, z2], axis=1)],\n",
    "          axis=0)\n",
    "\n",
    "        q_bar_score = (discriminator(q_bar)+.1) *.85 # +.1 * .85 so that it's 0<x<1\n",
    "        q_score = (discriminator(q)+.1) *.85 \n",
    "        tc_loss = K.log(q_score / (1 - q_score)) \n",
    "        discriminator_loss = - K.log(q_score) - K.log(1 - q_bar_score)\n",
    "    else:\n",
    "        tc_loss = 0\n",
    "        discriminator_loss = 0\n",
    "\n",
    "\n",
    "    reconstruction_loss = tf.keras.losses.mse(K.flatten(tg_inputs), K.flatten(tg_outputs)) \n",
    "    reconstruction_loss += tf.keras.losses.mse(K.flatten(bg_inputs), K.flatten(bg_outputs)) \n",
    "    reconstruction_loss *= input_shape[0] * input_shape[1] * input_shape[2]\n",
    "\n",
    "    kl_loss1 = 1 + tg_z_log_var - tf.keras.backend.square(tg_z_mean) - tf.keras.backend.exp(tg_z_log_var)\n",
    "    kl_loss2 = 1 + tg_s_log_var - tf.keras.backend.square(tg_s_mean) - tf.keras.backend.exp(tg_s_log_var)\n",
    "    kl_loss3 = 1 + bg_z_log_var - tf.keras.backend.square(bg_z_mean) - tf.keras.backend.exp(bg_z_log_var)\n",
    "\n",
    "    kl_loss1 = tf.keras.backend.sum(kl_loss1, axis=-1)\n",
    "    kl_loss2 = tf.keras.backend.sum(kl_loss2, axis=-1)\n",
    "    kl_loss3 = tf.keras.backend.sum(kl_loss3, axis=-1)\n",
    "\n",
    "    kl_loss = kl_loss1+kl_loss2+kl_loss3\n",
    "    #kl_loss = tf.keras.backend.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    \n",
    "    cvae_loss = tf.keras.backend.mean(reconstruction_loss + beta*kl_loss + gamma*tc_loss + discriminator_loss)\n",
    "    cvae.add_loss(cvae_loss)\n",
    "    \n",
    "    if type(opt)==type(None):\n",
    "        #print('optimizer not specified using ADAM, wroom wroom')\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate,beta_1=0.9,beta_2=0.999,epsilon=1e-07,amsgrad=False,name='Adam')\n",
    "        #opt = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9, momentum=0.9, epsilon=1e-07, centered=False, name='RMSprop')\n",
    "        #opt = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.1, nesterov=False, name='SGD')\n",
    "\n",
    "\n",
    "    cvae.compile(optimizer=opt,run_eagerly=True)\n",
    "    \n",
    "    return cvae, z_encoder, s_encoder, cvae_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d06a34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# params| 6,499,298\n"
     ]
    }
   ],
   "source": [
    "#import make_models;reload(make_models);from make_models import *\n",
    "batch_size = 32\n",
    "\n",
    "cvae, z_encoder, s_encoder, cvae_decoder = get_fMRI_CVAE_3D(input_shape=(64,64,1),\n",
    "                                                             latent_dim=[16,16],\n",
    "                                                             beta=.001,\n",
    "                                                             gamma=.001,\n",
    "                                                             disentangle=True,\n",
    "                                                             bias=True,\n",
    "                                                             batch_size = batch_size,\n",
    "                                                             kernel_size = 3,\n",
    "                                                             filters = 8,\n",
    "                                                             intermediate_dim = 128,\n",
    "                                                             nlayers = 6,\n",
    "                                                             strides = 2,\n",
    "                                                             learning_rate=0.001,\n",
    "                                                             opt=None)\n",
    "\n",
    "num_params = np.sum([np.prod(val.get_shape()) for val in cvae.trainable_weights])\n",
    "print(f'# params| {num_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cb99f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x155480059a90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvae.load_weights(os.path.join(save_dir,'cvae_weights'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45adce6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer zero_padding2d is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pad2d = tf.keras.layers.ZeroPadding2D(padding=((6,7),(6,7))) #If tuple of 2 tuples of 2 ints: interpreted as ((top_pad, bottom_pad), (left_pad, right_pad))\n",
    "data_loader = cvae_data_loader(cmats=pad2d(cmats[:,:,:,np.newaxis])[:,:,:,0].numpy(), df=df, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7910f452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvae_predict(z,s,asd):\n",
    "    \n",
    "    assert z.shape[0]==s.shape[0],'bad'\n",
    "    \n",
    "    if np.array(asd).ndim==0:\n",
    "        asd = np.repeat(asd,z.shape[0])\n",
    "        \n",
    "    z_ = np.zeros(s.shape)\n",
    "    s[~asd,:] = 0\n",
    "    \n",
    "    l = np.hstack((z,s))    \n",
    "    recon = cvae_decoder.predict(l)\n",
    "    \n",
    "    return recon[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86e7f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = df['diag'].values==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3927e9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 64, 64, 1) for input Tensor(\"tg_inputs:0\", shape=(None, 64, 64, 1), dtype=float32), but it was called on an input with incompatible shape (None, 51, 51, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 64, 64, 1) for input Tensor(\"tg_inputs:0\", shape=(None, 64, 64, 1), dtype=float32), but it was called on an input with incompatible shape (None, 51, 51, 1).\n",
      "CPU times: user 2.64 s, sys: 664 ms, total: 3.3 s\n",
      "Wall time: 474 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Z_mu,Z_sigma,Z = z_encoder.predict(cmats)\n",
    "S_mu,S_sigma,S = s_encoder.predict(cmats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bce94e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.97it/s]\n",
      "100%|██████████| 100/100 [00:12<00:00,  8.25it/s]\n"
     ]
    }
   ],
   "source": [
    "Z_sample100 = np.array([z_encoder.predict(cmats)[2] for _ in tqdm(range(100))])\n",
    "S_sample100 = np.array([s_encoder.predict(cmats)[2] for _ in tqdm(range(100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbbe2c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_td_mu = cvae_predict(Z_mu[~patients,:],S_mu[~patients,:],asd=False)\n",
    "recon_asd_mu = cvae_predict(Z_mu[patients,:],S_mu[patients,:],asd=True)\n",
    "recon_twin_mu = cvae_predict(Z_mu[patients,:],S_mu[patients,:],asd=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "771e6c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:23<00:00,  4.20it/s]\n",
      "100%|██████████| 100/100 [00:18<00:00,  5.39it/s]\n",
      "100%|██████████| 100/100 [00:18<00:00,  5.39it/s]\n"
     ]
    }
   ],
   "source": [
    "recon_td_samples = np.array([cvae_predict(Z_sample100[i,~patients,:],S_sample100[i,~patients,:],asd=False) for i in tqdm(range(Z_sample100.shape[0]))])\n",
    "recon_asd_samples = np.array([cvae_predict(Z_sample100[i,patients,:],S_sample100[i,patients,:],asd=True) for i in tqdm(range(Z_sample100.shape[0]))])\n",
    "recon_twin_samples = np.array([cvae_predict(Z_sample100[i,patients,:],S_sample100[i,patients,:],asd=False) for i in tqdm(range(Z_sample100.shape[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6aed4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "results['Z_mu'] = Z_mu\n",
    "results['Z_sigma'] = Z_sigma\n",
    "results['Z'] = Z\n",
    "results['S_mu'] = S_mu\n",
    "results['S_sigma'] = S_sigma\n",
    "results['S'] = S\n",
    "results['Z_sample100'] = Z_sample100\n",
    "results['S_sample100'] = S_sample100\n",
    "results['recon_td_mu'] = recon_td_mu\n",
    "results['recon_asd_mu'] = recon_asd_mu\n",
    "results['recon_twin_mu'] = recon_twin_mu\n",
    "results['recon_td_samples'] = recon_td_samples\n",
    "results['recon_asd_samples'] = recon_asd_samples\n",
    "results['recon_twin_samples'] = recon_twin_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dadee363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:27<00:00, 10.55s/it]\n"
     ]
    }
   ],
   "source": [
    "# Save the files\n",
    "[np.savez_compressed(os.path.join(save_dir,key+'.npz'),data=results[key]) for key in tqdm(list(results.keys()))];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ffe6cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 606 ms, sys: 1.71 s, total: 2.32 s\n",
      "Wall time: 2.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Dump as a pickle just in case\n",
    "import pickle\n",
    "with open((os.path.join(save_dir,'results.pickle')),'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbe4f8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 26s, sys: 852 ms, total: 2min 27s\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Save as a big numpy arr\n",
    "np.savez_compressed(file=os.path.join(save_dir,'results.npz'),\n",
    "                    Z_mu=Z_mu,\n",
    "                    Z_sigma=Z_sigma,\n",
    "                    Z=Z,\n",
    "                    S_mu=S_mu,\n",
    "                    S_sigma=S_sigma,\n",
    "                    S=S,\n",
    "                    Z_sample100=Z_sample100,\n",
    "                    S_sample100=S_sample100,\n",
    "                    recon_td_mu=recon_td_mu,\n",
    "                    recon_asd_mu=recon_asd_mu,\n",
    "                    recon_twin_mu=recon_twin_mu,\n",
    "                    recon_td_samples=recon_td_samples,\n",
    "                    recon_asd_samples=recon_asd_samples,\n",
    "                    recon_twin_samples=recon_twin_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33580231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Assets/tf_weights/CVAE_2022-03-25 18:28:49.469238/009-train-CVAE-cmats.ipynb'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "nb_name = '009-train-CVAE-cmats.ipynb'\n",
    "shutil.copyfile(src=nb_name,dst=os.path.join(save_dir,nb_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b4a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc1862ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr  1 05:34:15 EDT 2022\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cad192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
